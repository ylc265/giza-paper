\section{Introduction}

\subsection{Cross-DC Erasure Coding: Why Now?}

As the entire IT industry is rapidly moving to the cloud, more and more data centers are being built all over the globe. The event that one of the data centers will fail catastrophically gradually becomes inevitable. In other words, this is no longer a matter of ``if'', but rather ''when''. It is essential that data stored in the cloud is protected against such catastrophical failure.

There have been a long line of prior work~\ref{bib:BlahBlah} arguing for storing data objects in erasure coded form, as opposed to replication, across geo-graphically distributed data centers. Similar to geo-replication, cross-DC erasure coding can ensure durability even in the event of catastrophical data center failure. A main motivation for cross-DC erasure coding is to significantly reduce storage cost compared to geo-replication. The same economic force that has powered cloud providers to erasure code data within individual data centers natually extends to the cross-DC scenario.

Nevertheless, none of the cloud providers today offer options to erasure code customer data across data centers. This is largely due to the prohibitive cost of WAN bandwdith. The reduction in storage cost of cross-DC erasure coding comes at the additional cost of inflated WAN traffic, for both normal read/write work flow and access/rebuild in the event of data center failure. With geo-replication, normal read operation is served from single primary data center, so there is no WAN traffic. In the event of data center failure, all reads can completely fail-over to the secondary data center, where there is again no stress on the WAN backbone. With cross-DC erasure coding, however, both normal reads and fail-over reads incur non-trivial WAN traffic. Since the total cost of cloud storage includes both the storage alone cost and the WAN cost, cross-DC erasure coding has not been an economic solution.

Nevertheless, the technology advancement in wide area networking has progressed to a point where new innovation is greatly reducing WAN bandwidth cost. There are two key driving forces. The erbium-doped fiber amplifiers~\ref{bib:mears} makes it possible to amplifies a huge spectrum of optical signal directly, with the need to first convert it to an electrical signal. This enables Dense Wave Divsion Multiplexing (DWDM), which make it possible to send 10+ terabits per single fiber~\ref{bib:below}. 

\comment{

Mears, R.J. and Reekie, L. and Poole, S.B. and Payne, D.N.: ``Low-threshold tunable CW and Q-switched fiber laser operating at 1.55Î¼m'', Electron. Lett., 1986, 22, pp.159-160

Zhu, B., et al. ``112-Tb/s space-division multiplexed DWDM transmission with 14-b/s/Hz aggregate spectral efficiency over a 76.8-km seven-core fiber.'' Optics Express 19.17 (2011): 16665-16671.

}

Most recently, Facebook Inc. and Microsoft Corp. have teamed up to build a new fiber optic cable under the Atlantic Ocean, which uses eight pairs of fiber optic strands and comes online in 2017 with 160 terabits per second capacity~\ref{bib:MAREA1, bib:MAREA2}.

{\bf TODO: make the below a table}

\begin{table}[thp]
\centering
\begin{tabular}{|l|c|c|}
\hline
Cable Name                      & FLAG Atlantic 1~\ref{bib:FA-1}    & MAREA~\ref{bib:MAREA1, bib:MAREA2}
\\ \hline \hline
Ready For Service               & 2001                              & 2017
\\ \hline
Cost (Billion)                  & 1.1                               & undisclosed
\\ \hline
Capacity                        & 10 Gbps                           & 160 Tbps
\\ \hline \hline
\end{tabular}
\caption{TransAtlantic Networking Connectivity \\ (exemplary udersea cable connecting US and Europe)}
\label{tab:mears}
\end{table}

\comment{bib:MAREA1, http://www.wsj.com/articles/facebook-and-microsoft-to-build-fiber-optic-cable-across-atlantic-1464298853}
\comment{bib:MAREA2, http://www.usatoday.com/story/experience/2016/05/26/microsoft-facebook-undersea-cable-google-marea-amazon/84984882/}
\comment{bib:FA-1, https://en.wikipedia.org/wiki/Fiber-Optic_Link_Around_the_Globe}

\subsection{Giza Overview}

Giza provides an externally consistent (linearizable~\ref{bib:linearizable}) versioned object store, which erasure codes data objects and stores them across globally distributed data centers.

Customers access Giza service by creating special Giza storage accounts, which is similar to today's cloud storage accounts. In addition to storing data within single data center, across multiple availability zones within a region, or across two geo-graphically distributed data centers, Giza storage accounts allow the customers to specify the set of data centers where their data is striped across, as well as the resulted total storage overhead.

The customers access Giza service with simple put/get/delete interface. In addition to standard object storage interface, Giza provides addtional option to support versioning, where new put requests will not overwrite an existing object, but rather create new versions of the same object. The old versions remain in the system and accessible until they are explicitly removed.

Giza operates on the top of existing cloud storage systems and leverages their public APIs. It uses blob storage to store customer data objects and nosql table storage to store metatadata.

Figure~\ref{fig:giza_example} illustrates the work flow of putting a customer data object. Giza operates a group of {\em stateless} Giza nodes in every data center. Say a customer uses Giza clients (command line, library for various programming language, or REST interface) to put a 4MB data object. The Giza client routes the request to one of the Giza nodes in the data center closest to the customer. The Giza node divides the data object into 2 data fragments ($a$ and $b$), with each fragment being 2MB. It then invokes an erasure coding process and generates a parity fragment of 2MB. The Giza node disperses and stores the 3 fragments (2 data and 1 parity) in the blob storage in 3 data centers (1 local and 2 remote). The pointers to the 3 blob storage, together with versioning information, form the metadata of the customer data object. The Giza node persists the metadata in the nosql table storage in the 3 data centers.

\subsection{Challenges and Contributions}

Giza tolerates data center failures. Even in the event of massive data center failure, the customers need to be able to continuously get existing data objects, update them, or put new ones.

Giza supports concurrent gets and puts. While the blob storage and nosql table storage within individual data centers operate independently, Giza needs to coordinate all the accesses so as to achieve external consistency (linerizability).

The workloads that Giza target don't have high concurrency. This means that single reader / writer is the common case and Giza strives to {\em make the common case fast}. On the other hand, concurrency do arise in various situations, such as failures or uncommon usage. Even though concurrency is a rare case, Giza needs to {\em guarantee the rare case correct}. The technical challengue turns out to be how to achieve both the above, even in the event of massive data center failure.

Towards this end, we have made the following contributions:
\begin{itemize}
    \item We have designed and implemented Giza, which provides a versioned object store that erasure codes data objects and stores them across globally distributed data centers.
    \item Giza is fast in the common case: when there is no concurrency, Giza completes within single WAN RTT, which is optimal given the requirement to tolerate data center failure.
    \item Giza is correct in the rare case: under concurrency and with data center failure, Giza achieves external consistency (linearizability).
    \item Giza employs well-known distributed algorithms, such as Paxos and Fast Paxos, in a novel way so that it operates on the top of exsiting public cloud storage systems.
    \item Giza is deployed in xxx data centers. Trace-driven experiments demonstrate that Giza achieve all our design goals. In particular, it is worth pointing out that Giza achieves much lower latency than naively adopting a globally consistent storage system, like Google's spanner.
\end{itemize}
