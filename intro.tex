\section{Intro}
Giza is a cloud storage front end that erasure codes large mutable data files across multiple data centers . The motivation of Giza is to reduce overall storage cost and to tolerate entire data center failures, while maintaining strong consistency and reasonable read and write latencies.

\par
We have made the following contributions in this paper. First, we observed that the growth in network bandwidth will soon surpass the growth in storage capacity and that cross WAN erasure coding can and should be realized in practice. Second, we have designed a cross WAN erasure coding key value storage which relies on limited interface supported by many existing cloud APIs such as Azure and AWS. This key value store is strongly consistent and can tolerate complex network partitions. Lastly, we have implemented this system as a frontend on top of the Azure storage and demonstrated that such a system can replace current solutions for write heavy workloads.

\subsection{Background}
Paxos is a consensus algorithm that satisfies the safety property in an asynchronous network. 
\par
Erasure Encoding is a very mature technique used in storage systems for data striping and fault tolerance.

\subsection{}
Project Giza stores erasure coded data objects across multiple geographically distributed data centers. It provides the same or higher level of durability than geo-replication, but at much reduced storage cost. Project Giza leverages existing cloud storage APIs and operates on the top of existing public clouds.

\subsection{A Case for Flexible Cross-DC Erasure Coding}

Erasure coding across geo-graphically distributed data centers is a most effective approach to reduce storage cost while achieving the fault tolerance goal of being able to survive data center failure. As Facebook's F4 system~\ref{bib:F4} has demonstrated, replacing geo-replication with cross-DC erasure coding can effectively reduce storage overhead from 3.6x to 2.1x, achieving huge savings for Facebook's 65PB of worm storage. While a fixed 2 + 1 solution works very well for Facebook's special workload, the public cloud storage desires much more flexibility. Different customers have different desirable operating points in terms of cost, durability and latency trade-off and are willing to accept different pricing for different needs. This creates an opportunity to offer flexible cross-DC erasure coding options.

Giza provide completes flexibility to the customers. When a storage account is created, the customers have total freedom to specific which set of data centers, what type of erasure coding schemes and how much fault tolerance at the storage account level. In addition, the customer had additional flexibility to specific which data centers are involved, so that they could constraint all the data to be in the United States per data sovereignty requirement and regulation, or they could choose to disperse the erasure coded data across multiple continent, so that no single country could gain access to the complete data.

The default configuration applies $k+1$ erasure coding, where each data object is divided into $k$ data fragments and $1$ single parity fragment is generated from the $k$ data fragments. Here, all the $k+1$ fragments are persisted in $k+1$ data centers. In addition to the default configuration, the customer could 
The default configuration applies $k+1$ erasure coding, where each data object is divided into $k$ data fragments and $1$ single parity fragment is generated from the $k$ data fragments. Here, all the $k+1$ fragments are persisted in $k+1$ data centers. In addition to the default configuration, the customer could create storage accounts with {\em enhanced durability}, where 2 parity fragments are generated from the $k$ data fragments. The resulted $k+2$ erasure coding scheme would tolerate arbitrary 2 data center failures and therefore achieve much higher durability than the standard solution. As shown in the below table~\ref{tab:cost_benefit}, compared to the geo-replication, the enhanced durability is able to achieve much higher durability while still reducing storage cost.

{\b Daniel}, could you help to add the cost-benefit table from the slide deck here?

\subsection{}