\section{Intro}
Giza is a cloud storage front end that erasure codes large mutable data files across multiple data centers . The motivation of Giza is to reduce overall storage cost and to tolerate entire data center failures, while maintaining strong consistency and reasonable read and write latencies.

\par
We have made the following contributions in this paper. First, we observed that the growth in network bandwidth will soon surpass the growth in storage capacity and that cross WAN erasure coding can and should be realized in practice. Second, we have designed a cross WAN erasure coding key value storage which relies on limited interface supported by many existing cloud APIs such as Azure and AWS. This key value store is strongly consistent and can tolerate complex network partitions. Lastly, we have implemented this system as a frontend on top of the Azure storage and demonstrated that such a system can replace current solutions for write heavy workloads.

\subsection{Background}
Paxos is a consensus algorithm that satisfies the safety property in an asynchronous network. 
\par
Erasure Encoding is a very mature technique used in storage systems for data striping and fault tolerance.

\subsection{}
Project Giza stores erasure coded data objects across multiple geographically distributed data centers. It provides the same or higher level of durability than geo-replication, but at much reduced storage cost. Project Giza leverages existing cloud storage APIs and operates on the top of existing public clouds.

\subsection{Cross-DC Erasure Coding: Why Now?}

There has been a long line of prior work~\ref{bib:blah} arguing for storing data objects in erasure coded form, as opposed to replication, across geo-graphically distributed data centers. Similar to geo-replication, cross-DC erasure coding can ensure durability even in the event of massive data center failures. Cross-DC erasure coding, however, can result in lower storage cost compared to geo-replication. The same economis force that has powered clous storage providers to erasure code data within individual  data centers extends natually to the cross-DC scenario.

Nevertheless, none of the cloud storage providers today offer options for customers to erasure code their data across data centers to reduce cost. We argue this is largely due to an overlooked factor: the cost of WAN bandwdith. The reduction in storage cost of cross-DC erasure coding comes at the cost of inflated WAN traffic during normal read/write work flow, as well as in the event of massive data center failure. With geo-replication, normal read operation is served from single primary data center, so there is no WAN traffic. In the event of data center failure, all reads can completely fail-over to the secondary datan center, where there is again no stress on the WAN backbone. With cross-DC erasure coding, however, both normal reads and fail-over reads incur non-trivial WAN traffic. Since the total cost of cloud storage includes both the storage alone cost and the WAN cost, cross-DC erasure coding has not been an economic solution.

The technology advancement in wide area networking has progressed to a point where new innovation is greatly reducing WAN bandwidth cost. Most recently, Facebook Inc. and Microsoft Corp. have teamed up to build a new fiber optic cable under the Atlantic Ocean, which uses eight pairs of fiber optic strands and comes online in 2017 with 160 terabits per second capacity~\ref{bib:MAREA1, bib:MAREA2}.

TransAtlantic Undesea Cable     | FLAG Atlantic 1~\ref{bib:FA-1}    | MAREA~\ref{bib:MAREA1, bib:MAREA2}
Ready For Service (RFS)         | 2001                              | 2017
Cost (Billion)                  | 1.1                               | undisclosed
Capacity                        | 10 Gbps                           | 160 Tbps

{\comment bib:MAREA1, http://www.wsj.com/articles/facebook-and-microsoft-to-build-fiber-optic-cable-across-atlantic-1464298853}
{\comment bib:MAREA2, http://www.usatoday.com/story/experience/2016/05/26/microsoft-facebook-undersea-cable-google-marea-amazon/84984882/}
{\comment bib:FA-1, https://en.wikipedia.org/wiki/Fiber-Optic_Link_Around_the_Globe}

\subsection{Cross-DC Erasure Coding: A Case for Flexibility}

Erasure coding across geo-graphically distributed data centers is a most effective approach to reduce storage cost while achieving the fault tolerance goal of being able to survive data center failure. As Facebook's F4 system~\ref{bib:F4} has demonstrated, replacing geo-replication with cross-DC erasure coding can effectively reduce storage overhead from 3.6x to 2.1x, achieving huge savings for Facebook's 65PB of worm storage. While a fixed 2 + 1 solution works very well for Facebook's special workload, the public cloud storage desires much more flexibility. Different customers have different desirable operating points in terms of cost, durability and latency trade-off and are willing to accept different pricing for different needs. This creates an opportunity to offer flexible cross-DC erasure coding options.

Giza provide completes flexibility to the customers. When a storage account is created, the customers have total freedom to specific which set of data centers, what type of erasure coding schemes and how much fault tolerance at the storage account level. In addition, the customer had additional flexibility to specific which data centers are involved, so that they could constraint all the data to be in the United States per data sovereignty requirement and regulation, or they could choose to disperse the erasure coded data across multiple continent, so that no single country could gain access to the complete data.

The default configuration applies $k+1$ erasure coding, where each data object is divided into $k$ data fragments and $1$ single parity fragment is generated from the $k$ data fragments. Here, all the $k+1$ fragments are persisted in $k+1$ data centers. In addition to the default configuration, the customer could 
The default configuration applies $k+1$ erasure coding, where each data object is divided into $k$ data fragments and $1$ single parity fragment is generated from the $k$ data fragments. Here, all the $k+1$ fragments are persisted in $k+1$ data centers. In addition to the default configuration, the customer could create storage accounts with {\em enhanced durability}, where 2 parity fragments are generated from the $k$ data fragments. The resulted $k+2$ erasure coding scheme would tolerate arbitrary 2 data center failures and therefore achieve much higher durability than the standard solution. As shown in the below table~\ref{tab:cost_benefit}, compared to the geo-replication, the enhanced durability is able to achieve much higher durability while still reducing storage cost.

{\bf Daniel}, could you help to add the cost-benefit table from the slide deck here?

Discuss storage & bandwdith trade-off

\subsection{Alternative Approach}

To store a data object, Giza splits the object into multiple data fragments and generate parity fragments from the data fragments. Both data and parity fragments are dispersed and persisted at different data centers. To serve the data object, Giza reads enouch data and parity fragments from multiple data centers and reconstructs the data object. Hence, all Giza reads incur WAN traffic.

This design decision is a delibrate choice after careful considration of alternative approaches. One viable alternative is to first aggregate data objects into logical volumes and erasure code across different volumes. For instance, data objects in data center A are aggregated into $vol_A$ and data objects in data center B into $vol_B$. Each of the volume is large, say in the order of 100GB. The erasure coding process then takes both $vol_A$ and $vol_B$,  generates a parity volume $vol_P$ and stores $vol_P$ in data center C. In this approach, reading individual data object occurs in its corresponding data center and avoids WAN traffic. The challenge, however, is to handle object deletion. When a data object is deleted from $vol_A$ in data center A, it needs to be sent across WAN so that it can be {\em canceled} from $vol_C$. This adds great engineering complexity.

Facebook F4 system adtops this approach and avoids the problem completely by not deleting data objects. In F4, every data object is encrypted with a unique key. When a data object in $vol_A$ is deleted, its unique key is destroyed while the encrypted data object remains in the volume. Such simplification suits Facebook very well, as it can afford to not reclaiming the storage space occupied by the  $6.8\%$ deleted data. This, unfortunately, is not an option for us. First, our workloads demonstrate much larger deletion rate. Employing same simplification would result in much higher cost for us. Secondly, not physically deleting customer data - even if encrypted - wouldn't meet the compliance requirements for many of our customers.

Given these considerations, Giza chooses to split individual data objects and incur WAN traffic and latency during object retrival. Since the workload Giza targets is rather cold (very small numbers of reads over a large corpus of data), this choices works out very well.