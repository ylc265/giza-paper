\subsection{Joint Optimization of Data and Meta-data Operations}

The naive version of \name first writes out fragments (data and parity), and
then writes out metadata, resulting in two or more cross-DC round trips. To
reduce latency, we optimize \name to execute the data and metadata paths in
parallel. This is potentially problematic because either the data or metadata
path could fail while the other one succeeds. Below, we describe how {\em put}
and {\em get} cope with this challenge and ensure end-to-end correctness.

{\bf The \emph{put} Operation:} After generating the coded fragments, \name
launches the data and metadata paths in parallel. In the common case, \name
waits for both the data and the metadata paths to finish before acknowledging
clients as well as replicating the commit confirmation. In other words, \name
ensures that {\tt known committed versions} only include those whose data and
metadata have both been successfully committed.

In one uncommon case, the data path succeeds, while the metadata path fails.
Now, the fragments stored in the cloud blobs become orphans.
Giza will eventually delete these fragments and reclaim storage through a cleaning process,
which first executes Paxos to update the current version to {\em no-op},
discovers the orphan fragments as not being referenced in the metadata store, 
and then removes the fragments from the corresponding blob storage in all the DCs.

In another uncommon case, the data path fails, but the metadata path succeeds.
This subtle case creates a challenge for the {\em get} operation, as addressed
next.

{\bf The \emph{get} Operation:}
A naive way to perform {\em get} is to first read the latest metadata and then retrieve the fragments.
To reduce latency, {\name} chooses an optimistic approach and parallelizes the metadata and the data paths.

For a {\em get} request, {\name} first reads the metadata table row from a local DC.
It obtains {\tt known committed versions}, 
as well as the names and locations of the fragments of the latest version.
\name immediately starts reading the fragments from the multiple data centers.
Separately, it launches a regular metadata read to validate that the version is indeed the latest.
If the validation fails, \name realizes there is a newer version.
It in turn has to redo the data path by fetching a different set of fragments.
This results in wasted efforts in its previous data fetch.
Such potential waste, however, only happens when there is concurrent writes on the same object,
which is rare.

Because the data and metadata paths are performed in parallel during {\em put},
it is possible (though rare) that the fragments for the latest committed version
have not been written to the blob storage at the time of read.
This happens if the metadata path in the {\em put} finishes before the data path,
or the metadata path succeeds while the data path fails.
In such case, {\name} needs to fall back to read the previous version,
as specified in {\tt known committed versions}.

\subsection{Deletion and Garbage Collection}
\label{sec:garbagecollection}

The \emph{delete} operation in \name is treated as a special update of the
object's metadata. When receiving a delete request
(for either the entire object or specific versions), \name executes the
metadata path and writes a new version indicating the deletion.
As soon as the metadata update succeeds, the deletion completes and is acknowledged.

\comment{
Other than handling the users' delete requests, \name also needs to support two
extra features managing the storage space: 1) trimming earlier versions and
2) erasing an entire object thoroughly from the storage. To control the cost of
table and storage service, \name limits the number of versions for each object.
Once the threshold is reached, writing a new version will trigger the garbage
collection that trims the earliest version. After an object is deleted and no
new writes refresh the object, the space for its metadata and data also needs to
be recycled.
}

The storage space occupied by deleted versions/objects is reclaimed through garbage collection.
\name garbage collection deletes the fragments from the blob storage and truncates the columns of the deleted versions
from the metadata table row. It follows three steps: 1) fetching the metadata
corresponding to the version to be garbage collected, 2) deleting the 
fragments in the blob storage, and 3) removing the columns of the deleted version from
the metadata table row. The second step has to occur before the third one in
case that the garbage collection process is interrupted and the fragments
may become ``orphans'' without proper metadata pointing to them in the table
storage.

Once all the versions of the object are deleted and garbage collected,
Giza needs to remove the corresponding metadata table rows from all the DCs.
This requires extra care, due to possible contention from a new \emph{put} request.
If the metadata table rows are removed brutally, the new {\em put} request 
may lead the system into an abnormal state.
For instance, the {\em put} request could start at a data center where the metadata table
row has already been removed. Giza would therefore assume that the object
never existed and choose the smallest version number. Committing
this version number is dangerous before the metadata table rows are removed from
\emph{all} the DCs, as this may result in inconsistency during future failure
recovery.

Therefore, {\name} resorts to a two-phase commit protocol to remove the metadata table rows.
In the first phase, it marks the rows in all the DCs as \texttt{confined}.
After this any other {\em get} or {\em put} operations are temporarily disabled
for this object. In the second phase, all the rows are actually removed
from the table storage. The disadvantage of this approach is obvious. It
requires all the data centers to be online. Data center failure or network
partition may pause the process and make the row unavailable (but can still
continue after data center recovers or network partition heals).  

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

