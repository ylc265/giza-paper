\section{A Case for Giza}

\subsection{Giza: Flexible Cross-DC erasure coding }

Erasure coding across geo-graphically distributed data centers is a most effective approach to reduce storage cost while achieving the fault tolerance goal of being able to survive data center failure. As Facebook's F4 system~\ref{bib:F4} has demonstrated, replacing geo-replication with cross-DC erasure coding can effectively reduce storage overhead from 3.6x to 2.1x, achieving huge savings for Facebook's 65PB of warm storage. While a fixed 2 + 1 solution works very well for Facebook's special workload, the public cloud storage desires much more flexibility. Different customers have different desirable operating points in terms of cost, durability and latency trade-off and are willing to accept different pricing for different needs. 

Giza provide completes flexibility to the customers. When a storage account is created, the customers may specify how much fault tolerance is desired at the storage account level. In addition, the customers had additional flexibility to specific which data centers are involved, so that they could constraint all the data to be in the United States per data sovereignty requirement and regulation, or they could choose to disperse the erasure coded data across multiple continent, so that no single country could gain access to the complete data. 

The fault tolerance requirement and the data center involved in turn determine the erasure coding scheme applied. The default configuration applies $k+1$ erasure coding, where each data object is divided into $k$ data fragments and $1$ single parity fragment is generated from the $k$ data fragments. Here, all the $k+1$ fragments are persisted in $k+1$ data centers. In addition to the default configuration, the customer could create storage accounts with {\em enhanced durability}, where 2 parity fragments are generated from the $k$ data fragments. The resulted $k+2$ erasure coding scheme would tolerate arbitrary 2 data center failures and therefore achieve much higher durability than the standard solution. As shown in the below table~\ref{tab:cost_benefit}, compared to the geo-replication, the enhanced durability is able to achieve much higher durability while still reducing storage cost.

\subsection{Storage \& Bandwdith Trade-off}

{\bf add the cost-benefit table from the slide deck here}

\subsection{Alternative Approach}

To store a data object, Giza splits a data object into multiple data fragments plus generated parity fragments. Each data and parity fragments is dispersed and persisted at a different data center. To serve the data object, Giza reads enouch data and parity fragments from multiple data centers and reconstructs the data object. Hence, all Giza reads incur WAN traffic.

This design decision is a delibrate choice after careful considration of alternative approaches. One viable alternative is to first aggregate data objects into logical volumes and erasure code across different volumes. For instance, data objects in data center A are aggregated into $vol_A$ and data objects in data center B into $vol_B$. Each of the volume is large, say in the order of 100GB. The erasure coding process then takes both $vol_A$ and $vol_B$,  generates a parity volume $vol_P$ and stores $vol_P$ in data center C. In this approach, reading individual data object occurs in its corresponding data center and avoids WAN traffic. The challenge, however, is to handle object deletion. When a data object is deleted from $vol_A$ in data center A, it needs to be sent across WAN so that it can be {\em canceled} from $vol_C$. This adds engineering complexity. Another potential issue is on the fault tolerance garantee. If the customers need to protect against $r$ data center failure, the data object needs to be sent to $r-1$ data centers, which may increase WAN traffic.  

Facebook F4 system adtops the above approach as: 1) never delete data, and 2) only protect against one data center failure. In F4, every data object is encrypted with a unique key. When a data object in $vol_A$ is deleted, its unique key is destroyed while the encrypted data object remains in the volume. Such simplification suits Facebook very well, as it can afford to not reclaiming the storage space occupied by the  $6.8\%$ deleted data. This, unfortunately, is not an option for the general cloud storage. Our workloads shows much higher deletion rate. Employing the same simplification would result in non-trivial storage cost waste as the storage space of the deleted object is not reclaimed. Secondly, not physically deleting customer data - even if encrypted - wouldn't meet the compliance requirements for many of our customers.

Given these considerations, Giza chooses to split individual data objects and incur WAN traffic and latency during object retrival. Since the workload Giza targets is rather cold (very small numbers of reads over a large corpus of data), this choices works out well.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
