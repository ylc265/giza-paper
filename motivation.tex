\section{A Case for Giza}

\subsection{The Need for Flexibility}

Erasure coding across geo-graphically distributed data centers is a most effective approach to reduce storage cost while achieving the fault tolerance goal of being able to survive data center failure. As Facebook's F4 system~\ref{bib:F4} has demonstrated, replacing geo-replication with cross-DC erasure coding can effectively reduce storage overhead from 3.6x to 2.1x, achieving huge savings for Facebook's 65PB of worm storage. While a fixed 2 + 1 solution works very well for Facebook's special workload, the public cloud storage desires much more flexibility. Different customers have different desirable operating points in terms of cost, durability and latency trade-off and are willing to accept different pricing for different needs. This creates an opportunity to offer flexible cross-DC erasure coding options.

Giza provide completes flexibility to the customers. When a storage account is created, the customers have total freedom to specific which set of data centers, what type of erasure coding schemes and how much fault tolerance at the storage account level. In addition, the customer had additional flexibility to specific which data centers are involved, so that they could constraint all the data to be in the United States per data sovereignty requirement and regulation, or they could choose to disperse the erasure coded data across multiple continent, so that no single country could gain access to the complete data.

The default configuration applies $k+1$ erasure coding, where each data object is divided into $k$ data fragments and $1$ single parity fragment is generated from the $k$ data fragments. Here, all the $k+1$ fragments are persisted in $k+1$ data centers. In addition to the default configuration, the customer could 
The default configuration applies $k+1$ erasure coding, where each data object is divided into $k$ data fragments and $1$ single parity fragment is generated from the $k$ data fragments. Here, all the $k+1$ fragments are persisted in $k+1$ data centers. In addition to the default configuration, the customer could create storage accounts with {\em enhanced durability}, where 2 parity fragments are generated from the $k$ data fragments. The resulted $k+2$ erasure coding scheme would tolerate arbitrary 2 data center failures and therefore achieve much higher durability than the standard solution. As shown in the below table~\ref{tab:cost_benefit}, compared to the geo-replication, the enhanced durability is able to achieve much higher durability while still reducing storage cost.

\subsection{Storage & Bandwdith Trade-off}

{\bf add the cost-benefit table from the slide deck here}

\subsection{Alternative Approach}

To store a data object, Giza splits the object into multiple data fragments and generate parity fragments from the data fragments. Both data and parity fragments are dispersed and persisted at different data centers. To serve the data object, Giza reads enouch data and parity fragments from multiple data centers and reconstructs the data object. Hence, all Giza reads incur WAN traffic.

This design decision is a delibrate choice after careful considration of alternative approaches. One viable alternative is to first aggregate data objects into logical volumes and erasure code across different volumes. For instance, data objects in data center A are aggregated into $vol_A$ and data objects in data center B into $vol_B$. Each of the volume is large, say in the order of 100GB. The erasure coding process then takes both $vol_A$ and $vol_B$,  generates a parity volume $vol_P$ and stores $vol_P$ in data center C. In this approach, reading individual data object occurs in its corresponding data center and avoids WAN traffic. The challenge, however, is to handle object deletion. When a data object is deleted from $vol_A$ in data center A, it needs to be sent across WAN so that it can be {\em canceled} from $vol_C$. This adds great engineering complexity.

Facebook F4 system adtops this approach and avoids the problem completely by not deleting data objects. In F4, every data object is encrypted with a unique key. When a data object in $vol_A$ is deleted, its unique key is destroyed while the encrypted data object remains in the volume. Such simplification suits Facebook very well, as it can afford to not reclaiming the storage space occupied by the  $6.8\%$ deleted data. This, unfortunately, is not an option for us. First, our workloads demonstrate much larger deletion rate. Employing same simplification would result in much higher cost for us. Secondly, not physically deleting customer data - even if encrypted - wouldn't meet the compliance requirements for many of our customers.

Given these considerations, Giza chooses to split individual data objects and incur WAN traffic and latency during object retrival. Since the workload Giza targets is rather cold (very small numbers of reads over a large corpus of data), this choices works out very well.