\section{Design Considerations}
\label{sec:motivation}

\comment{
\subsection{Giza: Flexible Cross-DC erasure coding }

Erasure coding across geo-graphically distributed data centers is a most effective approach to reduce storage cost while achieving the fault tolerance goal of being able to survive data center failure. As Facebook's F4 system~\ref{bib:F4} has demonstrated, replacing geo-replication with cross-DC erasure coding can effectively reduce storage overhead from 3.6x to 2.1x, achieving huge savings for Facebook's 65PB of warm storage. While a fixed 2 + 1 solution works very well for Facebook's special workload, the public cloud storage desires much more flexibility. Different customers have different desirable operating points in terms of cost, durability and latency trade-off and are willing to accept different pricing for individual needs. 

Giza provide completes flexibility to the customers. When a storage account is created, the customers may specify how much fault tolerance is desired at the storage account level. In addition, the customers had additional flexibility to specify which data centers are involved, so that they could constraint all the data to be in the United States per data sovereignty requirement and regulation, or they could choose to disperse the erasure coded data across multiple continents, so that no single country could gain access to the complete data. 


}

\subsection{Storage, Bandwidth and Durability}

Giza also allows the customers to choose erasure coding scheme, which results in different operating points in terms of storage overhead, bandwidth cost and durability. 

The default configuration applies $k+1$ erasure coding, where each data object is divided into $k$ data fragments and $1$ single parity fragment is generated from the $k$ data fragments. Here, all the $k+1$ fragments are persisted in $k+1$ data centers. In addition to the default configuration, the customer could create storage accounts with {\em enhanced durability}, where 2 parity fragments are generated from the $k$ data fragments. The resulted $k+2$ erasure coding scheme would tolerate arbitrary 2 data center failures and therefore achieve much higher durability than the standard solution eventhough it has a much lower storage cost. 

The cost-benefit analysis of Giza can be shown in Table~\ref{tab:cost_benefit}.
To tolerate single data center failure, geo-replication requires storage overhead of $2\times1.3$x = 2.6x (where single DC storage overhead is 1.3x). With $k+1$ erasure coding, where $k$ ranges from 2 to 5, Giza reduces the storage overhead from 1.9x to 1.5x, a reduction of 27\% to 42\%. In addition to comparable durability as geo-replication, the customers could instead choose $k+2$ erasure coding, which tolerates two data center failures and achieves much higher durability. Yet, the storage overhead is much less than geo-replication, at 2.1x to 1.9x (a reduction of 19\% to 27\%). As summarized Table~\ref{tab:cost_benefit}, compared to geo-replication, Giza achieves comparable durability with significantly reduced storage overhead, or much higher durability with substantial reduction in storage overhead.

\subsubsection{Same WAN Traffic for Put}

Nevertheless, the reduction in storage overhead comes at the additional cost of inflated WAN traffic, for both read/write during normal operation and access/rebuild during the event of data center failure.

Let's revisit the example of putting a 4MB data object. In geo-replication, in addition to storing the object in a local DC, the same object is replicated across WAN and stored in a remote DC. Therefore, this incurs 1x of WAN traffic. Giza, with $2 + 1$ erasure coding, divides the data object into $a$, $b$ and generates parity $p$ with 2MB each. It stores $a$ in the local DC and $b$, $p$ in two remote DCs. Note that the amount of WAN traffic is again 1x. Same argument applies to other $k+1$ erasure coding scheme. Hence, the cross-WAN traffic is the same for both cross-DC erasure coding and geo-replication.

The analysis is readily extensible to $k+2$ schemes. As shown in Table~\ref{tab:cost_benefit}, the cross-WAN traffic of cross-DC erasure coding is slightly higher than geo-replication, which is inevitable for achieving the much higher durability.

\subsubsection{Inflated WAN Traffic for Get and Rebuild}

In geo-replication, since each DC has the full copy of the data object. A {\em get} request can access the local DC and incur no WAN traffic. On the other hand, with $2+1$ erasure coding, Giza can only obtain half the object from the local DC. Accessing the other half from a remote DC always incurs WAN traffic, which leads 0.5x unit of WAN traffic for the {\em put} request. Generalizing the argument, Table~\ref{tab:cost_benefit} shows the higher $k$ is (in $k+1$ erasure coding), the higher WAN traffic {\em get} incurs.

After a data center fails catastrophically, customer data originally stored at the failed DC needs to be rebuilt at a new DC. Geo-replication simply replicates every data object and thus incurs 1x of WAN traffic for rebuilding. Giza, on the other hand, requires erasure coding to reconstruct the missing fragment of every data object, which incurs $k$ times WAN traffic for all $k+r$ erasure coding schemes.

\begin{table*}[thp]
\centering
\begin{tabular}{|l||c||c|c|c|c||c|c|}
\hline
				& Geo-Replication    	& \multicolumn{4}{c||}{Giza (standard durability)}		& \multicolumn{2}{c|}{Giza (ultra durability)}
\\ \hline \hline
Number of DCs 				& 2										& 3 & 4 & 5 & 6									& 5 & 6
\\ \hline
Erasure coding scheme & replication					& 2 + 1 & 3 + 1 & 4 + 1 & 5 + 1	& 3 + 2 & 4 + 2
\\ \hline \hline
Storage overhead			& 2.6x								& 1.9x & 1.7x & 1.6x & 1.5x			& 2.1x & 1.9x
\\ \hline
Reduction							& -										& 27\% & 35\% & 38\% & 42\%			& 19\% & 27\%
\\ \hline \hline
WAN traffic (put)			& 1x									& 1x & 1x & 1x & 1x 						& 1.33x & 1.25x
\\ \hline
WAN traffic (get)			& 0										& 0.5x & 0.67x & 0.75x & 0.8x		& 0.67x & 0.75x
\\ \hline
DC rebuild 						& 1x									& 2x & 3x & 4x & 5x 						& 3x & 4x
\\ \hline \hline
\end{tabular}
\caption{Trade-off of storage, bandwidth and durability.}
\label{tab:cost_benefit}
\end{table*}

\subsection{Alternative Approach}
\label{sec:alternative}

\ch{It feels better to move this subsection to a later section dedicated for discussion. We could mention Haibo's work there, or better in the related work section. If we do that, the tile of this section should probably be changed to ``costs and benefits''.}

Giza treats data objects independently. To store one single data object, Giza splits the object into multiple data fragments and generates parity fragments. All the fragments are dispersed and stored in different data centers. To serve the data object, Giza reads enough data and parity fragments from the multiple data centers and reconstructs the data object. Hence, Giza always incurs WAN traffic when serving data.

This decision to treat data objects independently is a deliberate choice after careful considerations of alternative approaches. One viable alternative is to first aggregate data objects into logical volumes and erasure code across different volumes. For instance, data objects in data center A are aggregated into $vol_A$ and those in data center B into $vol_B$. Volumes are large, say in the order of 100GB. A parity volume $vol_P$ is generated by erasure coding $vol_A$ and $vol_B$, which is stored in yet another data center C.

This approach avoids WAN traffic when serving data, as every data object is available in its entirety in one of the DCs. However, the challenge of this approach is to handle object deletion. Whenever a data object is deleted from $vol_A$ in data center A, it needs to be transmitted across WAN so as to be {\em canceled} from $vol_C$, which incurs WAN traffic upon deletion. In addition to the WAN traffic, deleting objects from the logical volumes would inevitably requires additional bookkeeping and garbage collection, which would result in greatly increased engineering complexity.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
