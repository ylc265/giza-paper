\section{Motivation}
\label{sec:motivation}

\comment{
\subsection{Giza: Flexible Cross-DC erasure coding }

Erasure coding across geo-graphically distributed data centers is a most effective approach to reduce storage cost while achieving the fault tolerance goal of being able to survive data center failure. As Facebook's F4 system~\ref{bib:F4} has demonstrated, replacing geo-replication with cross-DC erasure coding can effectively reduce storage overhead from 3.6x to 2.1x, achieving huge savings for Facebook's 65PB of warm storage. While a fixed 2 + 1 solution works very well for Facebook's special workload, the public cloud storage desires much more flexibility. Different customers have different desirable operating points in terms of cost, durability and latency trade-off and are willing to accept different pricing for different needs. 

Giza provide completes flexibility to the customers. When a storage account is created, the customers may specify how much fault tolerance is desired at the storage account level. In addition, the customers had additional flexibility to specific which data centers are involved, so that they could constraint all the data to be in the United States per data sovereignty requirement and regulation, or they could choose to disperse the erasure coded data across multiple continent, so that no single country could gain access to the complete data. 

The fault tolerance requirement and the data center involved in turn determine the erasure coding scheme applied. The default configuration applies $k+1$ erasure coding, where each data object is divided into $k$ data fragments and $1$ single parity fragment is generated from the $k$ data fragments. Here, all the $k+1$ fragments are persisted in $k+1$ data centers. In addition to the default configuration, the customer could create storage accounts with {\em enhanced durability}, where 2 parity fragments are generated from the $k$ data fragments. The resulted $k+2$ erasure coding scheme would tolerate arbitrary 2 data center failures and therefore achieve much higher durability than the standard solution. As shown in the below table~\ref{tab:cost_benefit}, compared to the geo-replication, the enhanced durability is able to achieve much higher durability while still reducing storage cost.
}

\subsection{Storage, Bandwidth and Durability}

Giza gives the customers the flexibility to choose erasure coding scheme, which results in different operating points in terms of storage overhead, bandwidth cost and durability. 

To tolerate single data center failure, geo-replication requires storage overhead of $2\times1.3$x = 2.6x (where single DC storage overhead is 1.3x). With $k+1$ erasure coding, where $k$ ranges from 2 to 5, Giza reduces the storage overhead from 1.9x to 1.5x, a reduction of 27\% to 42\%. In addition to comparable durability as geo-replication, the customers could instead choose $k+2$ erasure coding, which tolerates two data center failures and achieves much higher durability. Yet, the storage overhead is much less than geo-replication, at 2.1x to 1.9x (a reduction of 19\% to 27\%). As summarized Table~\ref{tab:cost_benefit}, compared to geo-replication, Giza achieves comparable durability with significantly reduced storage overhead, or much higher durability with substantial reduction in storage overhead.

Nevertheless, the reduction in storage overhead comes at the additional cost of inflated WAN traffic, for both read/write during normal operation and access/rebuild during the event of data center failure. 

\subsubsection{Same WAN Traffic for Put}

Let's revisit the example of putting a 4MB data object. In geo-replication, in addition to storing the object in a local DC, the same object is replicated across WAN and stored in a remote DC. Therefore, this incurs 1x of WAN traffic. Giza, with $2 + 1$ erasure coding, divides the data object into $a$, $b$ and generates parity $p$ with 2MB each. It stores $a$ in the local DC and $b$, $p$ in two remote DCs. Note that the amount of WAN traffic is again 1x. Same argument applies to other $k+1$ erasure coding scheme. Hence, the cross-WAN traffic is the same for both cross-DC erasure coding and geo-replication.

The analysis is readily extensible to $k+2$ schemes. As shown in Table~\ref{tab:cost_benefit}, the cross-WAN traffic of cross-DC erasure coding is slightly higher than geo-replication, which is inevitable for achieving the much higher durability.

\subsubsection{Inflated WAN Traffic for Get and Rebuild}

In geo-replication, since each DC has the full copy of the data object. A {\em get} request can the local DC and incur no WAN traffic. On the other hand, with $2+1$ erasure coding, Giza can only obtain half the object from the local DC. Accessing the other half from a remote DC always incurs WAN traffic, which leads 0.5x unit of WAN traffic for the {\em put} request. Generalizing the argument, Table~\ref{tab:cost_benefit} shows the higher $k$ is (in $k+1$ erasure coding), the higher WAN traffic {\em get} incurs.

After a data center fails catastrophically, customer data originally stored at the failed DC needs to be rebuilt at a new DC. Geo-replication simply replicates every data object and thus incurs 1x of WAN traffic for rebuilding. Giza, on the other hand, requires erasure coding reconstruction to rebuild the missing fragment of every data object, which incurs $k$ times WAN traffic for all $k+r$ erasure coding schemes.

\begin{table*}[thp]
\centering
\begin{tabular}{|l||c||c|c|c|c||c|c|}
\hline
				& Geo-Replication    	& \multicolumn{4}{c||}{Giza (standard durability)}		& \multicolumn{2}{c|}{Giza (ultra durability)}
\\ \hline \hline
Number of DCs 				& 2										& 3 & 4 & 5 & 6									& 5 & 6
\\ \hline
Erasure coding scheme & replication					& 2 + 1 & 3 + 1 & 4 + 1 & 5 + 1	& 3 + 2 & 4 + 2
\\ \hline \hline
Storage overhead			& 2.6x								& 1.9x & 1.7x & 1.6x & 1.5x			& 2.1x & 1.9x
\\ \hline
Reduction							& -										& 27\% & 35\% & 38\% & 42\%			& 19\% & 27\%
\\ \hline \hline
WAN traffic (put)			& 1x									& 1x & 1x & 1x & 1x 						& 1.33x & 1.25x
\\ \hline
WAN traffic (get)			& 0										& 0.5x & 0.67x & 0.75x & 0.8x		& 0.67x & 0.75x
\\ \hline
DC rebuild 						& 1x									& 2x & 3x & 4x & 5x 						& 3x & 4x
\\ \hline \hline
\end{tabular}
\caption{Trade-off of storage, bandwidth and durability.}
\label{tab:cost_benefit}
\end{table*}

\subsection{Alternative Approach}

To store a data object, Giza splits a data object into multiple data fragments plus generated parity fragments. Each data and parity fragments is dispersed and persisted at a different data center. To serve the data object, Giza reads enouch data and parity fragments from multiple data centers and reconstructs the data object. Hence, all Giza reads incur WAN traffic.

This design decision is a delibrate choice after careful considration of alternative approaches. One viable alternative is to first aggregate data objects into logical volumes and erasure code across different volumes. For instance, data objects in data center A are aggregated into $vol_A$ and data objects in data center B into $vol_B$. Each of the volume is large, say in the order of 100GB. The erasure coding process then takes both $vol_A$ and $vol_B$,  generates a parity volume $vol_P$ and stores $vol_P$ in data center C. In this approach, reading individual data object occurs in its corresponding data center and avoids WAN traffic. The challenge, however, is to handle object deletion. When a data object is deleted from $vol_A$ in data center A, it needs to be sent across WAN so that it can be {\em canceled} from $vol_C$. This adds engineering complexity. Another potential issue is on the fault tolerance garantee. If the customers need to protect against $r$ data center failure, the data object needs to be sent to $r-1$ data centers, which may increase WAN traffic.  

Facebook F4 system adtops the above approach as: 1) never delete data, and 2) only protect against one data center failure. In F4, every data object is encrypted with a unique key. When a data object in $vol_A$ is deleted, its unique key is destroyed while the encrypted data object remains in the volume. Such simplification suits Facebook very well, as it can afford to not reclaiming the storage space occupied by the  $6.8\%$ deleted data. This, unfortunately, is not an option for the general cloud storage. Our workloads shows much higher deletion rate. Employing the same simplification would result in non-trivial storage cost waste as the storage space of the deleted object is not reclaimed. Secondly, not physically deleting customer data - even if encrypted - wouldn't meet the compliance requirements for many of our customers.

Given these considerations, Giza chooses to split individual data objects and incur WAN traffic and latency during object retrival. Since the workload Giza targets is rather cold (very small numbers of reads over a large corpus of data), this choices works out well.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
