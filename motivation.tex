\section{The Case for Giza}
\label{sec:motivation}

This section presents the characteristics of a large-scale cloud drive service with hundreds of millions of users. These characteristics motivate the design choices of Giza.
%The section is concluded with a discussion of an alternative approach.

\subsection{Cloud Drive Characteristics}

\begin{figure}[tp]
%\centering
\hspace{-4em}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{data/object_size-storage_capacity}
  \caption{}
  \label{fig:object_size-storage_capacity}
\end{subfigure}%
%\includegraphics[width=0.5\textwidth]{data/object_size-storage_capacity}
%\hspace{-.5em}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{data/write_read_gap-bytes_read}
  \caption{}
  \label{fig:write_read_gap-bytes_read}
\end{subfigure}%
\caption{Cloud Drive Characteristics}
\label{fig:case_for_giza}
\end{figure}

{\bf Methodology:} The data presented in this section is derived from a three-month trace of the cloud drive service. The cloud drive serves hundreds of millions of users and stores their objects, including documents, photos, music, videos and more. The trace includes {\em all} reads, writes and updates to {\em all} objects between January 1 and March 31, 2016.

{\bf Large Objects Dominate:} The size of the objects varies significantly, ranging from Kilobytes to tens of Gigabytes. While the total number of small objects vastly exceeds that of large objects, the storage capacity occupied by the small objects turns out extremely small. Indeed, Figure~\ref{fig:object_size-storage_capacity} presents the cumulative distribution of storage capacity consumption in terms of object size. We observe that less than $0.9\%$ of the total storage capacity is occupied by objects smaller than 4MB. This suggests that, to optimize storage cost, it is sufficient for Giza to focus on objects of 4MB and larger~\footnote{Objects of tens of Gigabytes are divided into 4MB chunks before storing in cloud storage back-end.}. For the same reason, objects smaller than 4MB are filtered in the following analysis.

{\bf Object Temperature Drops Fast:} A common usage scenario of cloud drive is sharing. Objects stored in the cloud drive are often shared across multiple devices, as well as among multiple users. Therefore, it is typical to observe reads soon after the objects are created. To this end, Figure~\ref{fig:write_read_gap-bytes_read} presents the cumulative distribution of bytes read in terms of object age when the reads occur~\footnote{The analysis focuses on all the objects created during the three-month period. Hence, the object age is capped at three months.}. It is worth pointing out that $47\%$ of the bytes read occurred in the same day of object creation, $87\%$ occurred within the same week, and merely less $2\%$ occurred beyond one month. This suggests that the temperature of the objects drops at a fast space. Moreover, caching the objects for a short period of time can satisfy most of the reads (more below).

\begin{table}[h]
\centering
\begin{tabular}{|c||c|c|}
\hline \hline
total reads (B) / writes (B) 	& \multicolumn{2}{c|}{2.3$\times$}
\\ \hline \hline
%\multirow{4}{*}{cross-DC reads / writes \newline in Giza}
	& no caching		& 1.15$\times$
\\ \cline{2-3}
cross-DC reads / writes
	& caching (day)		& 0.61$\times$ 
\\ \cline{2-3}
with Giza
	& caching (week)	& 0.18$\times$ 
\\ \cline{2-3}
	& caching (month)	& 0.05$\times$ 
\\ \hline \hline
\end{tabular}
%\caption{Giza with Caching in Local DC.}
\label{tab:caching}
\end{table}
{\bf Writes Dominate with Caching:} The above table presents the effectiveness of caching. The ratio between the total amount of bytes due to reads and writes is 2.3$\times$. This implies that on average each object is read 2.3 times. As illustrated in Section~\ref{subsec:trade-off}, Giza incurs 1x and 0.5x cross-DC network traffic in writes and reads, respectively. Hence, the ratio between cross-DC traffic due to reads and writes is $1.15\times$. Given the temperature analysis, it is most effective for Giza to additionally cache entire objects for a short period of time within one single DC. Serving reads from the caching DC dramatically reduces the cross-DC traffic due to reads. Indeed, when objects are cached for one day, the cross-DC traffic ratio due to reads and writes reduces to 0.61$\times$. When objects are cached for one month, the ratio reduces to negligible 0.05$\times$, where the cross-DC traffic is completely dominated by writes.

\begin{table}[h]
\centering
\begin{tabular}{c||c|c|c}
\# of Versions 	& 	1				& 2					& $\ge 3$
\\ \hline
Percentage			& $57.96\%$	& $40.88\%$	& $1.16\%$
\end{tabular}
\label{tab:version}
\end{table}
{\bf Concurrency Rare, but Versioning Required:} The above table presents how often objects are updated and require versioning support. We observe that $57.96\%$ of the objects are written once and never updated during the three-month period. For the remaining, $40.88\%$ of the objects are updated exactly once and merely $1.16\%$ are updated more than twice. This suggests that concurrent updates of objects are rare in Giza (albeit possible). Hence, Giza focuses and optimizes for single writer, while at the same time supports versioning.

%\subsection{Giza: Flexible Cross-DC erasure coding }
%
%Erasure coding across geo-graphically distributed data centers is a most effective approach to reduce storage cost while achieving the fault tolerance goal of being able to survive data center failure. As Facebook's F4 system~\ref{bib:F4} has demonstrated, replacing geo-replication with cross-DC erasure coding can effectively reduce storage overhead from 3.6x to 2.1x, achieving huge savings for Facebook's 65PB of warm storage. While a fixed 2 + 1 solution works very well for Facebook's special workload, the public cloud storage desires much more flexibility. Different customers have different desirable operating points in terms of cost, durability and latency trade-off and are willing to accept different pricing for individual needs. 
%
%Giza provide completes flexibility to the customers. When a storage account is created, the customers may specify how much fault tolerance is desired at the storage account level. In addition, the customers had additional flexibility to specify which data centers are involved, so that they could constraint all the data to be in the  United States per data sovereignty requirement and regulation, or they could choose to disperse the erasure coded data across multiple continents, so that no single country could gain access to the complete data. 

%\begin{table*}[tp]
%\centering
%\begin{tabular}{|l||c||c|c|c|c||c|c|}
%\hline
				%& Geo-Replication    	& \multicolumn{4}{c||}{Giza (standard durability)}		& \multicolumn{2}{c|}{Giza (enhanced durability)}
%\\ \hline \hline
%Number of DCs 				& 2										& 3 & 4 & 5 & 6									& 5 & 6
%\\ \hline
%Erasure coding scheme & replication					& 2 + 1 & 3 + 1 & 4 + 1 & 5 + 1	& 3 + 2 & 4 + 2
%\\ \hline \hline
%Storage overhead			& 2.6x								& 1.9x & 1.7x & 1.6x & 1.5x			& 2.1x & 1.9x
%\\ \hline
%Reduction							& -										& 27\% & 35\% & 38\% & 42\%			& 19\% & 27\%
%\\ \hline \hline
%WAN traffic (put)			& 1x									& 1x & 1x & 1x & 1x 						& 1.33x & 1.25x
%\\ \hline
%WAN traffic (get)			& 0										& 0.5x & 0.67x & 0.75x & 0.8x		& 0.67x & 0.75x
%\\ \hline
%DC rebuild 						& 1x									& 2x & 3x & 4x & 5x 						& 3x & 4x
%\\ \hline \hline
%\end{tabular}
%\caption{Trade-off of storage, bandwidth and durability.}
%\label{tab:cost_benefit}
%\end{table*}

\begin{table}[tp]
\centering
\small
\begin{tabular}{|l||c||c|c|c|}
\hline
											& Geo-Rep.						& \multicolumn{3}{c|}{Giza}
\\ \hline \hline
\# of DCs 						& 2										& 3 		& 5 		& 7
\\ \hline
Erasure coding 				& -										& 2 + 1	& 4 + 1	& 6 + 1
\\ \hline \hline
Storage overhead			& 2.6									& 1.9 	& 1.6 	& 1.5
\\ \hline
{\bf Cost savings}		& -										& {\bf 27\%} 	& {\bf 38\%} 	& {\bf 42\%}
\\ \hline \hline
cross-DC traffic (put)& 1x									& 1x 		& 1x 		& 1x
\\ \hline
cross-DC traffic (get)& 0										& 0.5x 	& 0.75x & 0.83x
\\ \hline
DC rebuild 						& 1x									& 2x 		& 4x 		& 6x
\\ \hline \hline
\end{tabular}
\caption{Giza Trade-offs}
\label{tab:cost_benefit}
\end{table}


\subsection{Giza Trade-offs}
\label{sec:alternative}

Compared to geo-replication, Giza offers more flexible trade-offs in terms of storage cost and cross-DC network traffic.
Table~\ref{tab:cost_benefit} summarizes the costs and benefits of Giza at various operating points.

{\bf Storage Cost:}
To tolerate single data center failure, geo-replication incurs storage overhead of $2\times1.3$ = 2.6 (with single DC storage overhead at 1.3).
With $k+1$ erasure coding, where $k$ ranges from 2 to 6, Giza reduces the storage overhead to between 1.9 and 1.5, a reduction of 27\% to 42\%.
Nevertheless, the storage cost savings come at the inflated cross-DC traffic, which is examined next.

{\bf Cross-DC Traffic:} For writes, Giza does {\em not} incur any additional cross-DC traffic than geo-replication. Objects are uploaded to one DC and processed by Giza in the DC. With $k+1$ erasure coding, the objects are divided into $k$ data fragments, where one fragment is stored in the DC locally and the rest $k$ fragments stored in remote DCs. Hence, the ratio between cross-DC traffic and object size is $k/k = 1$, same as geo-replication.

For reads, however, Giza incurs additional cross-DC traffic. $k$ fragments are requires to serve reads, where one is from the DC locally and the rest $k-1$ from remote DCs. Hence, the ratio between cross-DC traffic and object size is $(k-1)/k$, which increases with $k$. In comparison, geo-replication serves reads entirely from the DC locally and incurs no cross-DC traffic at all.

Upon data center failure, data originally stored at the failed DC needs to be rebuilt at a new DC. Geo-replication simply replicates every object and thus incurs 1x of cross-DC traffic. In contrast, Giza applies erasure decoding to reconstruct missing fragments, each incurring $k$ times cross-DC traffic.

{\bf Alternative Approach:} Giza treats objects independently. To store a object, Giza splits the object into multiple data fragments and generates parity fragments. All the coded fragments are dispersed and stored in different data centers. To retrieve the object, Giza reads enough coded fragments from the multiple data centers and reconstructs the object. Hence, Giza always incurs cross-DC traffic when reading object.

The decision to treat objects independently is a deliberate choice after careful considerations of alternative approaches. One viable alternative is to first aggregate objects into logical volumes and then erasure code across different volumes. For instance, objects in data center A are aggregated into $vol_A$ and those in data center B into $vol_B$. Volumes are large, say in the order of 100GB. A parity volume $vol_P$ is generated by erasure coding $vol_A$ and $vol_B$, which is stored in yet another data center C.

This approach avoids cross-DC traffic when reading individual objects, as every object is available in its entirety in one of the DCs. However, the challenge of this approach is to handle object deletion. Whenever an object is deleted from $vol_A$ in data center A, it needs to be transmitted to data center C so as to be {\em canceled} from $vol_C$. Hence, object deletion incurs cross-DC traffic. In addition, deleting objects from the logical volumes inevitably requires additional bookkeeping and garbage collection, resulting in greatly increased engineering complexity.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
