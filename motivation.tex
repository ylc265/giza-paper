\section{Design Considerations}
\label{sec:motivation}

\comment{
\subsection{Giza: Flexible Cross-DC erasure coding }

Erasure coding across geo-graphically distributed data centers is a most effective approach to reduce storage cost while achieving the fault tolerance goal of being able to survive data center failure. As Facebook's F4 system~\ref{bib:F4} has demonstrated, replacing geo-replication with cross-DC erasure coding can effectively reduce storage overhead from 3.6x to 2.1x, achieving huge savings for Facebook's 65PB of warm storage. While a fixed 2 + 1 solution works very well for Facebook's special workload, the public cloud storage desires much more flexibility. Different customers have different desirable operating points in terms of cost, durability and latency trade-off and are willing to accept different pricing for different needs. 

Giza provide completes flexibility to the customers. When a storage account is created, the customers may specify how much fault tolerance is desired at the storage account level. In addition, the customers had additional flexibility to specific which data centers are involved, so that they could constraint all the data to be in the United States per data sovereignty requirement and regulation, or they could choose to disperse the erasure coded data across multiple continent, so that no single country could gain access to the complete data. 

The fault tolerance requirement and the data center involved in turn determine the erasure coding scheme applied. The default configuration applies $k+1$ erasure coding, where each data object is divided into $k$ data fragments and $1$ single parity fragment is generated from the $k$ data fragments. Here, all the $k+1$ fragments are persisted in $k+1$ data centers. In addition to the default configuration, the customer could create storage accounts with {\em enhanced durability}, where 2 parity fragments are generated from the $k$ data fragments. The resulted $k+2$ erasure coding scheme would tolerate arbitrary 2 data center failures and therefore achieve much higher durability than the standard solution. As shown in the below table~\ref{tab:cost_benefit}, compared to the geo-replication, the enhanced durability is able to achieve much higher durability while still reducing storage cost.
}

\subsection{Storage, Bandwidth and Durability}

Giza allows the customers to specify the set of data centers for each storage account. Given the flexibility, the customers may choose to store data in different DCs within one single country (or continent), due to the requirement and regulation of data sovereignty. Alternatively, they may choose to disperse data across DCs over multiple countries (or even multiple continents), so that no single country could gain access to the data in its entirety. 

Giza also allows the customers to choose erasure coding scheme, which results in different operating points in terms of storage overhead, bandwidth cost and durability. 

To tolerate single data center failure, geo-replication requires storage overhead of $2\times1.3$x = 2.6x (where single DC storage overhead is 1.3x). With $k+1$ erasure coding, where $k$ ranges from 2 to 5, Giza reduces the storage overhead from 1.9x to 1.5x, a reduction of 27\% to 42\%. In addition to comparable durability as geo-replication, the customers could instead choose $k+2$ erasure coding, which tolerates two data center failures and achieves much higher durability. Yet, the storage overhead is much less than geo-replication, at 2.1x to 1.9x (a reduction of 19\% to 27\%). As summarized Table~\ref{tab:cost_benefit}, compared to geo-replication, Giza achieves comparable durability with significantly reduced storage overhead, or much higher durability with substantial reduction in storage overhead.

Nevertheless, the reduction in storage overhead comes at the additional cost of inflated WAN traffic, for both read/write during normal operation and access/rebuild during the event of data center failure. 

\subsubsection{Same WAN Traffic for Put}

Let's revisit the example of putting a 4MB data object. In geo-replication, in addition to storing the object in a local DC, the same object is replicated across WAN and stored in a remote DC. Therefore, this incurs 1x of WAN traffic. Giza, with $2 + 1$ erasure coding, divides the data object into $a$, $b$ and generates parity $p$ with 2MB each. It stores $a$ in the local DC and $b$, $p$ in two remote DCs. Note that the amount of WAN traffic is again 1x. Same argument applies to other $k+1$ erasure coding scheme. Hence, the cross-WAN traffic is the same for both cross-DC erasure coding and geo-replication.

The analysis is readily extensible to $k+2$ schemes. As shown in Table~\ref{tab:cost_benefit}, the cross-WAN traffic of cross-DC erasure coding is slightly higher than geo-replication, which is inevitable for achieving the much higher durability.

\subsubsection{Inflated WAN Traffic for Get and Rebuild}

In geo-replication, since each DC has the full copy of the data object. A {\em get} request can the local DC and incur no WAN traffic. On the other hand, with $2+1$ erasure coding, Giza can only obtain half the object from the local DC. Accessing the other half from a remote DC always incurs WAN traffic, which leads 0.5x unit of WAN traffic for the {\em put} request. Generalizing the argument, Table~\ref{tab:cost_benefit} shows the higher $k$ is (in $k+1$ erasure coding), the higher WAN traffic {\em get} incurs.

After a data center fails catastrophically, customer data originally stored at the failed DC needs to be rebuilt at a new DC. Geo-replication simply replicates every data object and thus incurs 1x of WAN traffic for rebuilding. Giza, on the other hand, requires erasure coding reconstruction to rebuild the missing fragment of every data object, which incurs $k$ times WAN traffic for all $k+r$ erasure coding schemes.

\begin{table*}[thp]
\centering
\begin{tabular}{|l||c||c|c|c|c||c|c|}
\hline
				& Geo-Replication    	& \multicolumn{4}{c||}{Giza (standard durability)}		& \multicolumn{2}{c|}{Giza (ultra durability)}
\\ \hline \hline
Number of DCs 				& 2										& 3 & 4 & 5 & 6									& 5 & 6
\\ \hline
Erasure coding scheme & replication					& 2 + 1 & 3 + 1 & 4 + 1 & 5 + 1	& 3 + 2 & 4 + 2
\\ \hline \hline
Storage overhead			& 2.6x								& 1.9x & 1.7x & 1.6x & 1.5x			& 2.1x & 1.9x
\\ \hline
Reduction							& -										& 27\% & 35\% & 38\% & 42\%			& 19\% & 27\%
\\ \hline \hline
WAN traffic (put)			& 1x									& 1x & 1x & 1x & 1x 						& 1.33x & 1.25x
\\ \hline
WAN traffic (get)			& 0										& 0.5x & 0.67x & 0.75x & 0.8x		& 0.67x & 0.75x
\\ \hline
DC rebuild 						& 1x									& 2x & 3x & 4x & 5x 						& 3x & 4x
\\ \hline \hline
\end{tabular}
\caption{Trade-off of storage, bandwidth and durability.}
\label{tab:cost_benefit}
\end{table*}

\subsection{Alternative Approach}

Giza treats data objects independently. To store one single data object, Giza splits the object into multiple data fragments and generates parity fragments. All the fragments are dispersed and stored in different data centers. To serve the data object, Giza reads enough data and parity fragments from the multiple data centers and reconstructs the data object. Hence, Giza always incurs WAN traffic when serving data.

This decision to treat data objects independently is a deliberate choice after careful considerations of alternative approaches. One viable alternative is to first aggregate data objects into logical volumes and erasure code across different volumes. For instance, data objects in data center A are aggregated into $vol_A$ and those in data center B into $vol_B$. Volumes are large, say in the order of 100GB. A parity volume $vol_P$ is generated by erasure coding $vol_A$ and $vol_B$, which is stored in yet another data center C.

This approach avoids WAN traffic when serving data, as every data object is available in its entirety in one of the DCs. However, the challenge of this approach is to handle object deletion. Whenever a data object is deleted from $vol_A$ in data center A, it needs to be transmitted across WAN so as to be {\em canceled} from $vol_C$, which incurs WAN traffic upon deletion. In addition to the WAN traffic, deleting objects from the logical volumes would inevitably requires additional bookkeeping and garbage collection, which would result in greatly increased engineering complexity.

Facebook F4 system adopts the above approach, but avoids the deletion challenge by never deleting data objects. In F4, every data object is encrypted with a unique key. Whenever a data object in a volume needs to be deleted, its unique key is destroyed while the encrypted data object remains in the volume. This simplification suits Facebook very well, because its deleted data only accounts for $6.8\%$ of total storage and Facebook could afford not to reclaim the storage space~\ref{bib:f4}. This, unfortunately, is not an option for Giza, as our workloads show much higher deletion rate. Not reclaiming the physical storage space from deleted data objects would result in significant waste and completely void the gain from cross-DC erasure coding. Furthermore, not physically deleting customer data objects - even if encrypted - wouldn't meet the compliance requirements for many of our customers.

In comparison, Giza vastly simplifies the deletion problem by treating data objects independently. It deletes a data object by deleting all its data and parity fragments from individual DCs, which involves only tiny metadata traffic across WAN.
Given the necessity and simplicity to implement deletion, Giza chooses to split individual data objects and incur WAN traffic and latency when serving data. Since the target workload for Giza is rather cold (very small numbers of reads over a large corpus of data), the amount of WAN traffic due to serving data turns out only a small fraction compared to storing data.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
