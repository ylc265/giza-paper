\section{Related Work}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

{\bf Erasure Coding in Cluster Storage:}
Weatherspoon et al. establish a strong case for erasure coded storage systems~\ref{weatherspoon2002erasure}. They demonstrate that such systems consume an order of magnitude less bandwidth and storage to provide similar durability as replicated systems.

Erasure coding has long been applied in many large-scale distributed storage systems~\ref{bib:saito2004fab, bib:zhang2004repstore, haeberlen2005glacier, bib:abd2005ursa, welch2008scalable, sathiamoorthy2013xoring, zhang2016efficient}, including productions systems at Facebook~\ref{borthakur2010hdfs}, Google~\ref{fikes2010storage, ford2010availability} and Microsoft Azure~\ref{huang2012erasure}. These solutions generalize the RAID approach~\ref{patterson1988case, wilkes1996hp} to a distributed cluster setting. Giza synchronously replicates erasure coded data across WAN. Since the latency across WAN is much higher than in a cluster, Giza focuses on minimizing round trips so as to optimize latency. In addition, Giza provides globally consistent {\em get} and {\em put} with versioning support.

{\bf Erasure Coding in Wide Area Storage:}
OceanStore \ref{OceanStore, bib:pond} employs erasure coding and encryption to achieve a global-scale persistent storage. It assumes a fundamentally untrusted infrastructure. To achieve strong consistency, OceanStore serializes updates via a primary tier of replicas, a generalized form of primary. These primary replicas cooperate through a Byzantine agreement protocol to choose the final commit order for the updates. 
HAIL~\ref{Bowers:2009:HHI:1653662.1653686}, RACS~\ref{Abu-Libdeh:2010:RCC:1807128.1807165} and NCCloud~\ref{hu2012nccloud} all stripe and erasure code data across multiple cloud storage providers. 
HAIL~\ref{Bowers:2009:HHI:1653662.1653686} applies error correction coding within individual providers and erasure coding across them. It is designed to withstand Byzantine adversaries and unifies proofs of retrievability and failure recovery. 
In comparison, Giza operates in a trusted environment and employs a leaderless consensus protocol. Updates may originate from arbitrary data centers and still complete with optimal latency. There is no need to relay all the updates through the primaries.  Giza also exploits the trade-off among storage cost, network bandwidth and durability.

RACS~\ref{Abu-Libdeh:2010:RCC:1807128.1807165} 
builds a working system that is compatible with existing cloud storage clients and able to use multiple storage providers as back-ends.
It addresses the conflict caused by concurrent writers using Apache ZooKeeper~\ref{zookeeper}, where readers-writer locks are implemented at per-key granularity to synchronize distributed RACS proxies.
Giza, on the other hand, implements consensus algorithms for individual keys and achieves strong consistency without centralized coordinators.

NCCloud~\ref{hu2012nccloud} implements a class of functional regenerating codes~\ref{Dimakis07networkcoding} that optimize cross-WAN repair bandwidth when a data center fails. Giza considers the failure of a data center to be a rare event, and employs standard Reed-Solomon coding, which leads to more WAN traffic for catastraphic data center repair, but much simpler operation under normal workload.

Facebook F4~\ref{muralidhar2014f4} is a production warm blob storage system, storing over 65PBs of logical data. It applies erasure coding across data centers and reduces effective-replication-factor from 3.6 to 2.1. 
It adopts an erasure coding approach as discussed in Section.~\ref{sec:alternative}, but avoids the deletion challenge by never deleting data objects. In F4, every data object is encrypted with a unique key. Whenever a data object in a volume needs to be deleted, its unique key is destroyed while the encrypted data object remains in the volume. This simplification suits Facebook very well, because its deleted data only accounts for $6.8\%$ of total storage and Facebook could afford not to reclaim the storage space~\ref{bib:f4}. This, unfortunately, is not an option for Giza, as our workloads show much higher deletion rate. Not reclaiming the physical storage space from deleted data objects would result in significant waste and completely void the gain from cross-DC erasure coding. Furthermore, not physically deleting customer data objects - even if encrypted - wouldn't meet the compliance requirements for many of our customers.

In comparison, Giza vastly simplifies the deletion problem by treating data objects independently. It deletes a data object by deleting all its data and parity fragments from individual DCs, which involves only tiny metadata traffic across WAN.
Given the necessity and simplicity to implement deletion, Giza chooses to split individual data objects and incur WAN traffic and latency when serving data. Since the target workload for Giza is rather cold (very small numbers of reads over a large corpus of data), the amount of WAN traffic due to serving data turns out only a small fraction compared to storing data.

\comment{
There are two general approaches in applying erasure coding to storage.  One
is to generate coded fragments within each data object.  This is commonly used
to achieve redundancy within a single data center~\cite{facebook:f4}.  Another 
approach is to treat multiple data objects as a coding group and generate
parity blocks that combine multiple objects.  Facebook's cross-DC erasure
coding uses this scheme to code immutable blobs.  To handle mutable coded
blocks, prior work resort to a RAID-like approach~\cite{haibo:fast} of using
static coding groups comprising of fixed sized blocks. The RAID approach has
only been applied in cluster settings.
}

{\bf Separating Data and Metadata:}
It is common for a storage systems to separate data and metadata path, and design a separate metadata service to achieve better scalability, e.g., FARSITE~\ref{adya2002farsite} and Ceph~\ref{weil2006ceph}. Gnothi~\ref{wang2012gnothi} replicates metadata to all replicas while data blocks only to a subset of the replicas. Cocytus~\ref{zhang2016efficient} is a highly available in-memory KV-store that applies replication to metadata and erasure coding to data so as to achieve memory efficiency.
Giza follows a similar design path, and store data in commodity cloud blob storage and metadata in commodity NoSQL table storage.

{\bf Consistency in Global Storage:}
Primary-backup, MDCC, Mencius, Spanner

