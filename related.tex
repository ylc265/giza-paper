\section{Related Work}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

{\bf Erasure Coding in Cluster Storage:}
Erasure coding has long been applied in many large-scale distributed storage
systems~\cite{fab:asplos04, zhang04repstore, haeberlen05glacier, abd05ursa,
  welch08scalable, sathiamoorthy13xoring, zhang16efficient}, including
productions systems at Facebook~\cite{borthakur2010hdfs},
Google~\cite{fikes2010storage, ford10availability} and Microsoft
Azure~\cite{huang12erasure}. These solutions generalize the RAID
approach~\cite{patterson88case, wilkes96hp} to a distributed cluster setting.
Giza is unique in synchronously replicating erasure coded data across WAN
and minimizing cross-DC latency. In addition,
Giza provides globally consistent {\em put} and {\em get} with versioning support.

{\bf Erasure Coding in Wide Area Storage:}
HAIL~\cite{hail:ccs09}, OceanStore~\cite{oceanstore:asplos00, pond:fast03},
RACS~\cite{racs:socc10}, DepSky~\cite{depsky:eurosys11} 
and NCCloud~\cite{nccloud:fast12} all stripe and erasure code data at a global scale.

HAIL~\cite{hail:ccs09} is designed to withstand Byzantine adversaries.
OceanStore~\cite{oceanstore:asplos00, pond:fast03} assumes untrusted infrastructure
and serializes updates via a primary tier of replicas.
Giza operates in a trusted environment.

RACS~\cite{racs:socc10} and DepSky~\cite{depsky:eurosys11} address conflicts
caused by concurrent writers using Apache ZooKeeper~\cite{zookeeper:atc10},
where readers-writer locks are implemented at per-key granularity for synchronization.
Giza, on the other hand, implements consensus algorithms for individual keys
and achieves strong consistency without centralized coordinators.
In addition, Giza employs a leaderless consensus protocol. 
Updates may originate from arbitrary data centers
and still complete with optimal latency without being relayed through a primary.

NCCloud~\cite{nccloud:fast12} implements a class of functional regenerating
codes~\cite{dimakis07networkcoding} that optimize cross-WAN repair bandwidth.
Giza employs standard Reed-Solomon coding and leaves such optimization to future.

Facebook f4~\cite{f4:osdi14} is a production warm blob storage system.
It applies erasure coding across data centers for storage efficiency.
As discussed in Section~\ref{sec:alternative}, f4 avoids the
deletion challenge by never truly deleting data objects. 
Whenever a data object is deleted, the unique key used to encrypt the object is destroyed
while the encrypted data remains in the system.
This simplification suits Facebook very well, because its deleted
data only accounts for $6.8\%$ of total storage and Facebook could afford not to
reclaim the storage space~\cite{f4:osdi14}. This, unfortunately, is not an
option for Giza, as our workloads show much higher deletion rate. Not reclaiming
the physical storage space from deleted data objects would result in significant
waste and completely void the gain from cross-DC erasure coding. Furthermore,
not physically deleting customer data objects - even if encrypted - wouldn't
meet the compliance requirements for many of our customers.

\comment{
In comparison, Giza vastly simplifies the deletion problem by treating data
objects independently. It deletes a data object by deleting all its data and
parity fragments from individual DCs, which involves only tiny metadata traffic
across WAN. Given the necessity and simplicity to implement deletion, Giza
chooses to split individual data objects and incur WAN traffic and latency when
serving data. Since the target workload for Giza is rather cold (very small
numbers of reads over a large corpus of data), the amount of WAN traffic due to
serving data turns out only a small fraction compared to storing data.

There are two general approaches in applying erasure coding to storage.  One
is to generate coded fragments within each data object.  This is commonly used
to achieve redundancy within a single data center~\cite{f4:osdi14}.  Another 
approach is to treat multiple data objects as a coding group and generate
parity blocks that combine multiple objects.  Facebook's cross-DC erasure
coding uses this scheme to code immutable blobs.  To handle mutable coded
blocks, prior work resort to a RAID-like approach~\cite{zhang16efficient} of using
static coding groups comprising of fixed sized blocks. The RAID approach has
only been applied in cluster settings.
}

{\bf Separating Data and Metadata:}
It is common for a storage systems to separate data and metadata path, and
design a separate metadata service to achieve better scalability, e.g.,
FARSITE~\cite{adya02farsite} and Ceph~\cite{weil06ceph}.
Gnothi~\cite{wang12gnothi} replicates metadata to all replicas while data blocks
only to a subset of the replicas. Cocytus~\cite{zhang16efficient} is a highly
available in-memory KV-store that applies replication to metadata and erasure
coding to data so as to achieve memory efficiency. Giza follows a similar design
path, and store data in commodity cloud blob storage and metadata in commodity
NoSQL table storage.

{\bf Consistency in Global Storage:} 
Megastore~\cite{baker11megastore} and
Spanner~\cite{spanner:osdi12} applies Multi-Paxos to maintain strong consistency
in global databases. Both of them requires two round trips for a slave site to
commit. Mencius~\cite{mao08mencius} takes a round-robin approach for proposers in
different sites, amortizing commit latency. EPaxos~\cite{epaxos:sosp13} uses
fine-grained dependency tracking at acceptor-side to ensure low commit latency
for both non-contended and contended requests. In comparison, 
\name takes a refined approach based on FastPaxos~\cite{lamport05fast},
separating metadata and data path before committing. This design choice allows
\name to serve most requests still in single cross-DC round trip
while keeping servers stateless, using the limited ability of table service.
Metasync~\cite{metasync:atc15} implements Paxos using the append functionality
provided by cloud file synchronization services such as DropBox, OneDrive. By
contrast, Giza implements Paxos using conditional-write APIs of cloud tables.
The latter leads to a more efficient implementation as clients do not need to
download and process logs from the cloud storage in order to execute Paxos.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

