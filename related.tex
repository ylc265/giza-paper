\section{Related Work}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

{\bf Erasure Coding in Cluster Storage:}
Weatherspoon et al. establish a strong case for erasure coded storage
systems~\cite{weatherspoon02erasure}. They demonstrate that such systems consume
an order of magnitude less bandwidth and storage to provide similar durability
as replicated systems.

Erasure coding has long been applied in many large-scale distributed storage
systems~\cite{fab:asplos04, zhang04repstore, haeberlen05glacier, abd05ursa,
  welch08scalable, sathiamoorthy13xoring, zhang16efficient}, including
productions systems at Facebook~\cite{borthakur2010hdfs},
Google~\cite{fikes2010storage, ford10availability} and Microsoft
Azure~\cite{huang12erasure}. These solutions generalize the RAID
approach~\cite{patterson88case, wilkes96hp} to a distributed cluster setting.
Giza synchronously replicates erasure coded data across WAN. Since the latency
across WAN is much higher than in a cluster, Giza focuses on minimizing round
trips so as to optimize latency. In addition, Giza provides globally consistent
{\em get} and {\em put} with versioning support.

{\bf Erasure Coding in Wide Area Storage:}
OceanStore \cite{oceanstore:asplos00, pond:fast03} employs erasure coding and
encryption to achieve a global-scale persistent storage. It assumes a
fundamentally untrusted infrastructure. To achieve strong consistency,
OceanStore serializes updates via a primary tier of replicas, a generalized form
of primary. These primary replicas cooperate through a Byzantine agreement
protocol to choose the final commit order for the updates. 
HAIL~\cite{hail:ccs09}, RACS~\cite{racs:socc10}, DepSky~\cite{depsky:eurosys11} and
NCCloud~\cite{nccloud:fast12} all stripe and erasure code data across multiple
cloud storage providers. HAIL~\cite{hail:ccs09} applies error correction coding
within individual providers and erasure coding across them. It is designed to
withstand Byzantine adversaries and unifies proofs of retrievability and failure
recovery. In comparison, Giza operates in a trusted environment and employs a
leaderless consensus protocol. Updates may originate from arbitrary data centers
and still complete with optimal latency. There is no need to relay all the
updates through the primaries. Giza also exploits the trade-off among storage
cost, network bandwidth and durability.

RACS~\cite{racs:socc10} builds a working system that is compatible with existing
cloud storage clients and able to use multiple storage providers as back-ends.
DepSky~\cite{depsky:eurosys11} presents an enhanced scheme by employing an
secret sharing scheme that provides confidentiality guarantee. Both of the
systems address the conflict caused by concurrent writers using Apache
ZooKeeper~\cite{zookeeper:atc10}, where readers-writer locks are implemented at
per-key granularity to synchronize distributed RACS proxies. Giza, on the other
hand, implements consensus algorithms for individual keys and achieves strong
consistency without centralized coordinators. 

NCCloud~\cite{nccloud:fast12} implements a class of functional regenerating
codes~\cite{dimakis07networkcoding} that optimize cross-WAN repair bandwidth
when a data center fails. Giza considers the failure of a data center to be a
rare event, and employs standard Reed-Solomon coding, which leads to more WAN
traffic for catastraphic data center repair, but much simpler operation under
normal workload.

Facebook f4~\cite{f4:osdi14} is a production warm blob storage system, storing
over 65PBs of logical data. It applies erasure coding across data centers and
reduces effective-replication-factor from 3.6 to 2.1. It adopts an erasure
coding approach as discussed in Section.~\cite{sec:alternative}, but avoids the
deletion challenge by never deleting data objects. In f4, every data object is
encrypted with a unique key. Whenever a data object in a volume needs to be
deleted, its unique key is destroyed while the encrypted data object remains in
the volume. This simplification suits Facebook very well, because its deleted
data only accounts for $6.8\%$ of total storage and Facebook could afford not to
reclaim the storage space~\cite{f4:osdi14}. This, unfortunately, is not an
option for Giza, as our workloads show much higher deletion rate. Not reclaiming
the physical storage space from deleted data objects would result in significant
waste and completely void the gain from cross-DC erasure coding. Furthermore,
not physically deleting customer data objects - even if encrypted - wouldn't
meet the compliance requirements for many of our customers.

In comparison, Giza vastly simplifies the deletion problem by treating data
objects independently. It deletes a data object by deleting all its data and
parity fragments from individual DCs, which involves only tiny metadata traffic
across WAN. Given the necessity and simplicity to implement deletion, Giza
chooses to split individual data objects and incur WAN traffic and latency when
serving data. Since the target workload for Giza is rather cold (very small
numbers of reads over a large corpus of data), the amount of WAN traffic due to
serving data turns out only a small fraction compared to storing data.

\comment{
There are two general approaches in applying erasure coding to storage.  One
is to generate coded fragments within each data object.  This is commonly used
to achieve redundancy within a single data center~\cite{f4:osdi14}.  Another 
approach is to treat multiple data objects as a coding group and generate
parity blocks that combine multiple objects.  Facebook's cross-DC erasure
coding uses this scheme to code immutable blobs.  To handle mutable coded
blocks, prior work resort to a RAID-like approach~\cite{zhang16efficient} of using
static coding groups comprising of fixed sized blocks. The RAID approach has
only been applied in cluster settings.
}

{\bf Separating Data and Metadata:}
It is common for a storage systems to separate data and metadata path, and
design a separate metadata service to achieve better scalability, e.g.,
FARSITE~\cite{adya02farsite} and Ceph~\cite{weil06ceph}.
Gnothi~\cite{wang12gnothi} replicates metadata to all replicas while data blocks
only to a subset of the replicas. Cocytus~\cite{zhang16efficient} is a highly
available in-memory KV-store that applies replication to metadata and erasure
coding to data so as to achieve memory efficiency. Giza follows a similar design
path, and store data in commodity cloud blob storage and metadata in commodity
NoSQL table storage.

{\bf Consistency in Global Storage:}
Megastore~\cite{baker11megastore} and Spanner~\cite{spanner:osdi12} applies
Multi-Paxos to maintain strong consisteny in global databases. Both of them
requires two roundtrips for a slave site to commit. Mencius~\cite{mao08mencius}
takes a round-robin approch for proposers in different site, amortizing the
commit latency. EPaxos~\cite{epaxos:sosp13} uses fine-grained dependency
tracking at acceptor-side to ensure low commit latency for both non-contended
and contended requests. In comparison to this series of work, \name takes a
refined approach based on FastPaxos~\cite{lamport05fast}, separating metadata
and data path before committing. This design choice allows \name to serve most
requests still in one roundtrip while keeping servers stateless, using the
limited ability of table service. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

