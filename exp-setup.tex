\begin{figure}
\footnotesize
\centering
\begin{tabular}{c|c|c|c}
          & Coding & Data and Metadata DCs & Ping (max)\\
\hline
%US-2-1 	  & 2 + 1 & 3 & Central, South Central, West & Central, South Central, West & 46 ms \\
%US-6-1 	  & 6 + 1 & 7 & Central, South Central, West & Central, South Central, West & 71 ms\\ 
%jWorld-2-1 & 2 + 1 & 3 & Central, Europe North, Japan East & Central, Europe North, Japan East & 240ms\\
%World-6-1 & 6 + 1 & 7 & Central, South Central, West & Central, Europe North, Japan East & 241 ms\\
US-2-1 	  & 2 + 1 & US(3/3)                   & 46 ms \\
US-6-1 	  & 6 + 1 & US(7/3)                   & 71 ms\\ 
World-2-1 & 2 + 1 & US(1/1), EU(1/1), JP(1/1) & 240 ms\\
World-6-1 & 6 + 1 & US(3/1), EU(2/1), JP(2/1) & 241 ms\\
\end{tabular}
\caption{Giza Configuration ( US(7/3) represents 7 DCs for data and 3 DCs for metadata, all in the US. )} 
\label{fig:dcconfig} 
\end{figure}

\subsection{Experimental Setup}
We run experiments using four configurations: US-2-1, World-2-1, US-6-1, and World-6-1. Figure~\ref{fig:dcconfig} describes the data centers participating in each configuration, and the max ping latency between the DCs. Unless explicitly stated, all experiments erasure code objects of 4MB, the dominating size in our target workloads.

We also compare Giza with CockroachDB~\cite{cockroachdb}, an open source implementation of Google spanner. Our CockroachDB experiments use the US-2-1 configuration as world wide replication is not supported at the time. In every data center, we run three CockroachDB instances for local DC replication. Each CockroachDB writes to a dedicated HDD with no memory caching. We have configured the CockroachDB instances following the recommended production setting by the CockroachDB developers. For example, we run NTP to synchronize clocks of the different CockroachDB instances.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

