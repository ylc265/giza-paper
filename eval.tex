\section{Evaluation}

For evaluation, we deploy Giza on top of the Microsoft Azure platform across 11 data centers (7 in North America, 2 in Europe and 2 in Asia). Giza nodes are Azure virtual machines with 16 cores, 56 GB of RAM, and Gigabit Ethernet. As describe in Section~\ref{sec:design}, all the Giza nodes are stateless. For each Giza storage account, a locally redundant Azure Blob and Table storage account is created in every DC. Upon receiving {\em get} or {\em put} requests, the Giza nodes execute the data and the metadata paths to read or write data objects.

\input{exp-setup}

\subsection{Metadata Latency}

Giza implements both Classic and Fast Paxos for its metadata path. Here, we compare the performance of the two algorithms and examine their effects on Giza metadata latency. Figure~\ref{fig:metadata} presents the metadata latencies and breakdowns for both US-2-1 and World-2-1 configurations. The results include running proposers in each of the DCs.

The metadata latency consists of three parts: query version latency, transfer latency and table latency.
The query version latency, a simple read from the proposer’s local table to determine the possible highest version, is not part of the consensus protocol and is the same for both Classic and Fast Paxos.
The transfer latency is the amount of time spent on network communication between the proposer and the furthest Giza proxy in a Paxos quorum. Here, the latency of Classic Paxos, which incurs two cross-DC round trips, is not strictly twice as much as the latency of Fast Paxos. This is because the quorum of Classic Paxos is smaller than that of Fast Paxos, which results in the {\em furthest} Giza proxy being closer in Class Paxos.
The table latency is the latency for a Giza proxy to conditionally update its local DC table. Since Classic Paxos requires two rounds and hence two table updates, its table latency is consistently around twice that of Fast Paxos.

%The metadata latency is first affected by the number of cross-DC round trips the Paxos algorithms require. Classic Paxos takes 2 cross-DC round trips and Fast Paxos only 1 round trip. The latency of single round trip consists of two parts: 1) transfer latency, which is the latency to relay the metadata request to a proxy Giza node in a remote DC; and 2) table latency, which is the latency to conditionally update the table locally in the remote DC.

%It turns out that the table latencies across various DCs are quite comparable, around 60 ms or so.
%The transfer latencies, however, vary significantly from 20 ms (US central to US south central) to 240 ms (east Japan to north Europe).
%
%The metadata latency is further affected by quorum size. Classic Paxos completes with 2 out of 3 
%
%Indeed, Figure~\ref{fig:metadata} presents the metadata latencies and breakdowns for the both US-2-1 and World-2-1 configurations.

%When comparing the performance between implementing the metadata path with Fast Paxos and Classic Paxos, we found two factors that are important in determining which configuration outperforms the other.  The first factor is whether the VM to local Azure write latency is dominant in the metadata path and the second factor is the difference in the maximal distance between data centers in a fast quorum and a classic quorum. 
%
%The performance of the metadata path is primarily determined by the request latency between the coordinator and the table storage of the furthest data center in the quorum. Each request to a remote table storage is broken down into two parts: VM to VM latency and VM to local Azure storage latency. From our previous analysis, the write latency from a VM to its local Azure storage is around 60 ms. The VM to VM latency is determined by the location of the data centers and can range from 20 ms (us central to us south central) to 240 ms (east Japan to north Europe). Figure~\ref{fig:metadata} illustrates the latency difference between the two Paxos schemes.

For the US-2-1 configuration, we observe that the metadata latency is dominated by table latency. In this case, Fast Paxos is much faster than Classic Paxos, regardless of the proposer locations.

For the World-2-1 configuration, transfer latency becomes a substantial part of the overall metadata latency. In this case, despite of taking two cross-DC round trips, the smaller quorum of Classic Paxos sometimes leads to lower transfer latency than Fast Paxos. Nevertheless, the table latency of Classic Paxos is still around twice that of Fast Paxos. In the end, Fast Paxos turns out still faster than Classic Paxos, regardless of the proposer locations.

%In addition, if the max distance between the data centers in a fast quorum is close to that in a classic quorum, the latency of the Fast Paxos metadata path is half of that of the Classic Paxos metadata path. In certain scenarios, it is possible for Classic Paxos to achieve lower latency than Fast Paxos. While we didn’t observe this in our two configurations, we see that if we start the request from North Europe in the 2-1 world configuration, the latencies are almost the same. This is because a Fast Paxos round requires a bigger quorum response and in this case, a fast quorum consists of North Europe, US, and East Japan. The VM to VM latency between North Europe and East Japan is 240 ms while the VM to VM latency between North Europe and US is only 102 ms. Hence the max distance between data centers in the classic quorum (US and North Europe) is less than half of that of the fast quorum. The scenario in which the Classic Paxos scheme outperforms the Fast Paxos scheme is:
%\[max\_dist(F) > 2 * max\_dist(Q) + \textrm{extra table access}
%\]
%where F is the fast quorum size and Q is the classic quorum size.
%\input{fig_metadata}


\subsection{\name Latency}

The design of Giza went through multiple iterations and this section illustrates the performance gain in latency for each iteration. For the interest of space, we focus on the 2-1-World configuration.
%where US Central generates all requests as representative of the performance gain seen in other configurations.

\subsubsection{Giza Put Latency}
%\subsubsection{Giza Latency}
\input{fig_lat}

Figure~\ref{fig:eval_giza_put} shows the \name overall {\em put} latency for 4MB data. We compare Giza with two previous iterations where the metadata path is not parallelized with the data path. In the first iteration, Giza runs the data path first. Upon success, Giza runs the metadata path with Classic Paxos implementation. In the second iteration, we replaced Classic Paxos with Fast Paxos and see the performance gain described previously. Giza parallelizes metadata path with data path, which can results in extra metadata or data clean up if one of them fails. However, the performance gain is significant. We also included a baseline which is the blob store latency from the requester (US Central) to the furthest data center (Japan East). Finally, we include the latency for storing the 4MB data directly to Azure storage, which is locally replicated.

The results show that \name's performance beats the other two alternatives in the common case and has closest
latency to the baseline. The median latency of \name put is 374 ms, which is only 30 ms
higher than the baseline. This is due to the latency of erasure coding 4mb data. On the other hand, the serial classic paxos version takes 
852 ms, and the serial fast paxos version takes 598 ms. Compared to local replication, the latency cost of tolerating data center failure with Giza is a little more than 3 times.

\subsubsection{Giza Get Latency}

Similar to the put test, we did a \name get test, as shown in Figure~\ref{fig:eval_giza_get}. The alternative design here is the non-optimistic {\em get } where the most current version for a blob is not assumed to be stored in the current data center. Hence, the metadata path and data path are executed sequentially. We see the performance gain from the pessimistic version with a median latency 419 ms to the optimistic version's 223 ms. Giza's get latency is higher than the baseline by 33 ms. The gap between \name and baseline is higher because in the get operation \name needs to do a local table retrieval first before starting the datapath. In addition, it needs to decode the data fragments. Here the latency cost of erasure encoding on the read path with Giza is roughly twice that of reading from a locally redundant Azure storage.

%The results are expected. The \name put latency consists of a 
%metadata put latency and datapath latency. 



\comment{

% \begin{figure}[!h]
% \centering
%   \subfloat[Giza Put 99th Percentile]{\includegraphics[width=0.5\textwidth]{images/write_latency}\label{fig:f1}}
%   \subfloat[Giza Get 99th Percentile]{\includegraphics[width=0.5\textwidth]{images/write_latency}\label{fig:f2}}
% \caption{Comparison of latency of the four configurations}
% \end{figure}
\subsection {Different Configurations}
\sm {
  In this section, I will provide a latency graph (put and get) of all the 4 different configurations. The x axis is the size of the objects and the y axis is the latency. This section is to illustrate the trade off between storage efficiency and read latency. 
}

}

\subsection{Footprint Impact}

\input{fig_four}

\name offers customers the flexibility to choose the set of data centers, as well as the erasure coding schemes. As the main goal of Giza is storage cost reduction, we want to encourage erasure encoding with more numbers of data fragments. Giza’s operation latency is determined by the max of the data path and the metadata path.  Since the data path is more often the dominant latency, erasure coding with higher number of data fragments can actually improve latency performance. This is because more data fragments means that less bytes are sent to each storage, reducing the overall data path latency. Figure~\ref{fig:eval_giza_put_four} and  Figure~\ref{fig:eval_giza_get_four} illustrate this in our four configurations with 4MB data, with all requests generated from US-Central. This improvement of performance with higher configurations occurs when the max distance between the data centers in the higher configuration group (7 data centers) is not more than the max distance between the data centers in the lower configuration group (3 data centers).


\subsection{Comparing Giza with CockroachDB}

Here, we compare the US-2-1 configuration with CockroachDB (World-2-1 is skipped as CockroachDB currently does not fully support worldwide replication). % We also don't include comparison to  the US-6-1 scheme in the interest of space.

We implemented Giza put and get as transactions. Because CockroachDB fully supports ACID transactions, we can use CockroachDB to build a strongly consistent key value store that erasure codes objects across multiple data centers. To do this, we create four different tables: one metadata table and three data tables that serve as key-value stores for coded fragments. The metadata table is replicated across all three data centers, once in each data center, and can tolerate one data center failure. Each of the data tables is replicated three times in its respective data center. % Since CockroachDB uses Paxos for replication, the write critical path only requires the completion of 2 our of 3 replications.
This is to match the local replication of Azure storage within individual DCs.

The implementation of Giza put as a transaction consists of storing each coded fragment at the corresponding data table and storing the metadata information in the metadata table. This transaction involves all four tables. Figure~\ref{fig:eval_cock_put} compares the latency performance of CockroachDB with Giza. We observe that Giza outperforms CockroachDB significantly.

One explanation of the huge latency difference could be that CockroachDB is currently not optimized to store large objects. To minimize such concern, we further experimented CockroachDB with 128KB objects. This is 64KB after encoding, a perfect size CockroachDB claims to handle propoerly. Still, the latency of 128KB objects (blue bar) turns out much worse than that of Giza even with 4MB objects.

The implementation of Giza get as transactions consists of reading the metadata from the metadata table and two coded fragments from the data tables. This transaction involves three out of the four tables. Figure~\ref{fig:eval_cock_get} shows the performance comparison. Here, CockroachDB achieves lower latency than Giza. This is, however, because CockroachDB reads directly from local HDD, which is faster than Giza reading from Azure storage. To demonstrate this, we equalize the storage layer and plot the imaginary Giza latency where the Azure storage latency is substituted by the local HDD latency. Indeed, Giza with equalized storage is faster than CockroachDB.

\input{fig_cock}

\subsection{Giza Contention}

Giza is optimized for low contention workloads. So, it employs a simple strategy for handling contention. In the event of contention, a Giza node that fails the fast round immediately switches to a classic round. In addition, Giza implements exponential back-off with the latency starting from the median cross-DC latency whenever prepare phase or accept phase further fails in the classic round.

Figure~\ref{fig:gizacontentionbar} compares the performance of Giza driven by the cloud drive traces to that with no contention at all. In the cloud drive trace, we observe that only 0.5\% of updates are concurrent (within 1 second interval). Hence, it is not surprise that the performance of Giza driven by the traces is almost identical to that with no contention.

Figure~\ref{fig:gizacontentionbar} also presents the latency results of adversary contention. In this case, two Giza nodes within the same data center are issuing back-to-back concurrent puts to update the same object. This is definitely not the scenario Giza targets. We include the results for the interest of our readers.

%In most cases, both the fast rounds fail. In this scenario, one of the the Giza node will fail either at the prepare phase or the accept phase. In the former case, the expected latency is 6 RTT: Fast round -> Prepare Phase -> Back off -> Prepare Phase -> Accept Phase -> Fast Round. In the latter case, the expected latency is 7 RTT: Fast Round -> Prepare Phase -> Accept Phase -> Back off -> Prepare Phase -> Accept Phase -> Fast Round. This is consistent with the median contention latency.

%It is important to note that while Giza is correct under contention, it is not optimized for contention. This is because Giza is designed to handle real life workload with little to no contentions. From Section~\ref{sec:motivation}, we observe that only 0.5\% of Giza's updates are concurrent (within 1 seconds). Figure~\ref{fig:gizacontentioncdf}  compares the estimated latency cdf of Giza under a no-contention workload with workload driven by Section~\ref{sec:motivation}. 

%We evaluate the latency performance of Giza’s metadata path under contention with our 2-1-US configuration. From our One Drive analysis, only 0.5\% of all updates happen concurrently. To simulate this workload, we run 1000 metadata put where 0.5\% of puts are concurrent puts from two Giza nodes. All requests are from the same data center (US Central). Figure~\ref{fig:gizacontentioncdf} shows the cdf of running Giza with One Drive’s workload distribution when compared with workload with no contentions. Since only a small number of puts are concurrent, the cdf of the contention workload follows the same curve as that of the no contention workload until 95\%. 

%During contention, there may be multiple interleaving of our metadata path protocol. In the best scenario, on one metadata path, the fast paxos round succeeds. The ideal scenario here is for the Giza node which failed the fast round to wait until the version number is updated. It will then run a new fast round for the new version. However it is possible for both fast found to fail. In this scenario, the version number never increases. In this case, the Giza nodes will have to run classical paxos. We use the same random back off used in other paxos system, taking into account the median latency for each Paxos phase. Figure~\ref{fig:gizacontentionbar}  illustrates the contention latency when compared to the no contention latency.

\input{fig_contention}

%We evaluate the throughput of Giza in a local-area cluster. Figure xxx shows the result of a single Giza deployment for storing 4MB objects. The 24 ops per second equates to 144MB per second which is consistent with the 150MB per second cap for each Azure Virtual Machine. For each additional Giza node, we add three storage accounts. This is to deal with the current throughput cap per storage account, which is 200MB per second.  The keyspace is partitioned equally among sets of three storage accounts. Figure~\ref{fig:gizathroughput} depicts the throughput as we increase Giza nodes. Here, the throughput increases linearly. 

%\begin{figure*}
%\begin{tabular}{c|c|c|c|c|c}
%Giza Nodes & 1 & 2 & 3 & 4 & 5\\
%\hline
%ops/second & 24 & 34 & 49 & 73 & 96 \\
%MB/second & 144 & 204 & 294 & 438 & 576\\ 
%\end{tabular}
%\caption{Throughput result of local Giza cluster\label{fig:gizathroughput}} 
%\end{figure*}

%To set up CockroachDB, we use the same Azure virtual machine instances and run a single CockroachDB node. We followed the recommended production settings by the developers of CockroachDB when deploying these instances. For example, on the same virtual machine, we also run NTP to provide moderately accurate time to preserve data consistency. Other optimizations can be found on the CockroachDB website. We only benchmark CockraochDB against Giza in the 3 dc cluster scenario since we want the fault tolerance level to be the same for the comparisons.
%Since variability in latency is a factor when benchmarking cloud storage, we run all our experiments at approximately the same time.
%Since latency is an issue, we run all our experiments at around the same time.
%We experimented with different erasure coding schemes
% For all experiments, we deployed a single virtual machine (16 cores, 56 GB of RAM, 800 GB SSD, and gigabit ethernet) for each geographical region. We use the same virtual machines for setting up the Cassandra and CockroachDB clusters. The client issuing the requests runs on one of the virtual machine that is also part of the cluster. 
% To set up Giza, we also had to deployed both a table service and a blob service provided by the cloud service platform. The granularity of replication for these services varies from provider to provider but we always choose the replication level to match that of the regional replication. This means that as long as there’s no dc outage, the data would not be lost. For each data center, we run a Giza node frontend with the virtual machine. The Giza node can service requests from a client running in the virtual machine. In addition, requests to its local table and blob storage from other Giza nodes also go through the Giza node frontend in the form of an RPC call. This is to avoid unecessary WAN round trips when dealing with complicated table and blob storage operations. 
%\input{fig_cock}

%\sm {
%  In this section, I will have two graphs. One comparing Giza's full write path with CockroachDB's full replication. Another one is comparing the metadata path with cockroach db. This is to show, hopefully, that the fast paxos scheme is better. I will probably have 3 graphs each making request from one of the three datacenters. Due to multipaxos, there might be extra latency for cockroachdb's case.
%}
%We benchmark the performance of Giza with CockroachDB in two cases. In the first case, we use CockroachDB as a geo-replicating blah blah blah. Here is the result.
%In the second case, we used cockroachdb's transaction to simulate what we are doing with Giza. Blah blah blah, here is the result.
%64K $\sim$ 16MB

%X-axis: Value size
%Y-axis: 50\% Read latency

%X-axis: Value size
%Y-axis: 90\% Read latency

%X-axis: Value size
%Y-axis: 99\% Read latency

%Same for write

%[adding cpu results in a table]

% \subsection{Large object}
% 256MB $\sim$ 1GB

% X-axis: Value size
% Y-axis: Average Read latency

% X-axis: Value size
% Y-axis: Average Write latency


% \subsection{Contention}

% Fixed object size
% X-axis: zipf coefficient
% Y-axis: 50\%, 90\%, 99\% Read/Write Latency


% \subsection{Real workload}
% Table.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

