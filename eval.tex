\section{Evaluation}

\comment{

Giza provides fault tolerance through erasure coding across wide area networks while providing linearizability for its read and write operations. As such, there are currently no system that is specifically designed to serve as an alternative to Giza. However, we benchmark Giza’s performance against Cassandra and CockroachDB to illustrate the following points:

- In the common case, Giza’s Fast Paxos implementation of the metadata phase allows for lower latency writes for Giza when compared to Cassandra.

- While one could implement Giza’s metadata phase and data phase as a single distributed transaction, we illustrate through CockroachDB that doing so will degrade performance significantly.

In addition, we provide evaluation results for encoding scheme ranging from 2-1 in 3 data centers to 18-4 in 22 data centers to illustrate the flexibility of Giza as a front end.

}

For evaluation, we deploy Giza on top of the Microsoft Azure platform. The deployment reaches xxx data centers (xxx in North America, xxx in Europe and xxx in Asia). In each data center, we create a set of virtual machines as Giza nodes. We also create a number of Azure storage accounts. These storage accounts are local to individual DCs and allow access to Azure Blob storage and Azure Table storage in the DCs.

We form \name storage account as a collection of the local Azure storage accounts. We also specify the erasure coding scheme for each account. For example, a North American only Giza storage account may apply $2+1$ erasure coding and consist of 4 data centers in the US, as xxx, xxx, xxx and xxx. A global Giza storage account may apply same coding and consist of 4 data centers across 3 continents, as xxx, xxx, xxx and xxx.

As described in section~\ref{sec:design}, all the Giza nodes are stateless. Upon receiving {\em get} or {\em put} requests, the Giza nodes execute the data and the metadata paths to read or write data objects.

\input{exp-setup}
\subsection{Metadata Path Optimization}
\sm {
  Hey Shuai, here I will provide the graph compaing the two metadata paths and the datapath to illustrate that this optimization makes the metadata path the non-dominant path in most cases. I will also have a table with the latency breakdown. The motivation for this is to show that such an optimization can allow users to tune the latency performance by changing the erasure coding scheme. This would not be possible it we use the two round trip. I will provide different latency measurements (95th, 75th, 50th percentile, as well as average with error bar so we can choose one later. This is true for all latency results I will provide)
}
We measure Giza's latency performance by running 1000 write and read operations with a single threaded Giza node.We vary the object size (1mb and 4mb) and compared different configurations. 
The 95th percentile latency result is shown in Figure 4. The four configurations we chose are 3 data centers in the US (2-1 erasure coding scheme), 3 data centers across the world, 7 data centers in the US (6-1 erasure coding scheme), and 7 data centers across the world. Both the configuration (i.e distance between data centers) and the size of the object impact latency peformance. Larger objects incur higher latency in both the vm to vm transfer and the local blob storage access. This can explain the similar put latency results between the 3 US data center configuration and the 7 US data center configuration. Even though in the 3 US data center configuration, the distance between the data centers are much shorter (Iowa, Illinois, and West Central), each data fragment is 2MB. We can see in Table 3 that this increases the latency performance for the data path. In the 7 US data center configuration, each data fragment is just 700 KB, which reduces latency even when the data centers are further away. 



\subsection{Decomposing \name Latencies}

In our workloads, the predominant object size is 4MB. Hence, we focus on the {\em put} and {\em get} latency of 4MB objects in the evaluation. For every data point reported in this section, we repeat the corresponding Giza operation 1000 times and then calculate the latency at desired percentile. The global Giza storage account with 4 DCs is used for this evaluation.

\subsubsection{Data Path Tunneling}

\Cref{fig:eval_tunneling} shows the latency comparison between data path with tunneling and 
directly accessing remote blob storage without tunneling. The median latency with tunneling 
is xxx ms, vs xxx ms without tunneling. In addition, the latency with tunneling is much 
more stable over time. Its (90\%, 10\%) latency is (xxx ms, xxx ms), while without tunneling 
is (xxx ms, xxx ms). The variance without tunneling probably comes from the blob storage 
has a huge public load and it varies over time. Although in practice \name without tunneling 
should bring the same level of performance improvement, we chose tunneling for the rest of 
the evaluation to have more analysis-friendly results.

\subsubsection{Metadata Path}

To show the effects of our fast path design of metadata, we mesured the metadata path latency 
for both fast path and slow path, as shown in~\Cref{fig:eval_fastpath}. As expected, the fast 
path takes one network roundtrip plus the table read/write latency (with median at xxx ms); 
the slow path takes two roundtrips, whose latency is about twice of fast path (with median at 
xxx ms). This test concludes that the fast path optimization is critical to \name overall 


\subsubsection{Giza Put Latency}
%\subsubsection{Giza Latency}
\input{fig_lat}

Figure~\ref{fig:eval_giza_put} shows the \name overall put latency for 4MB data. In comparison, 
we tested two possible alternative designs: a sequential datapath with slow metadata 
path (as normal Paxos); parallel datapath with slow metadata path. In addition, we also 
included a one across datacenter RPC plus blob storage access test as baseline (it does 
not gurantee any level of consistency).

The results show that \name's performance beats the other two alternatives and has closest
latency to the baseline. The median latency of \name put is xxx ms, which is only xxx ms
higher than the baseline. On the other hand, the serial version with only slow path takes 
xxx ms, and the parallel version with slow path takes xxx ms.

\subsubsection{Giza Get Latency}

Similar to the put test, we did a \name get test, as shown in Figure~\ref{fig:eval_giza_get}. 
\name also beats the other two alternatives, with a median latency xxx ms, higher than 
the baseline by xxx ms. The gap between \name and baseline is higher because in the get 
operation \name needs to do a local table retrieve first before starting the datapath. 

%The results are expected. The \name put latency consists of a 
%metadata put latency and datapath latency. 



\comment{

% \begin{figure}[!h]
% \centering
%   \subfloat[Giza Put 99th Percentile]{\includegraphics[width=0.5\textwidth]{images/write_latency}\label{fig:f1}}
%   \subfloat[Giza Get 99th Percentile]{\includegraphics[width=0.5\textwidth]{images/write_latency}\label{fig:f2}}
% \caption{Comparison of latency of the four configurations}
% \end{figure}
\subsection {Different Configurations}
\sm {
  In this section, I will provide a latency graph (put and get) of all the 4 different configurations. The x axis is the size of the objects and the y axis is the latency. This section is to illustrate the trade off between storage efficiency and read latency. 
}

}

\subsection{Footprint Impact}

\input{fig_four}

\name offers customers the flexibility to choose the set of data centers, as well as the 
erasure coding schemes. To show the performance of different configurations, we evaluate 
4 different \name storage accounts, with the cross combination of $2+1$ vs $6+1$ erasure 
coding and all data centers in North America vs across 3 continents.

Figure~\ref{fig:eval_giza_put_four} shows the put latency for different configurations; 
Figure~\ref{fig:eval_giza_get_four} shows the get latency. When the system is deployed within 
North America, it has more stable latency results with both coding schemes. The $6+1$ coding
is faster because it gives more parallelism for the data path. Because the data is fixed, 
having more datacenters means smaller data fragments. The world-wide deploy shows much 
more invariance in performance, which is expected because the system is facing the across-
continent internet traffic. The $6+1$ performs better in this setup, in terms of the 
both median latency and variance. 

\input{eval_cock}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

