\section{Evaluation}

\comment{

Giza provides fault tolerance through erasure coding across wide area networks while providing linearizability for its read and write operations. As such, there are currently no system that is specifically designed to serve as an alternative to Giza. However, we benchmark Giza’s performance against Cassandra and CockroachDB to illustrate the following points:

- In the common case, Giza’s Fast Paxos implementation of the metadata phase allows for lower latency writes for Giza when compared to Cassandra.

- While one could implement Giza’s metadata phase and data phase as a single distributed transaction, we illustrate through CockroachDB that doing so will degrade performance significantly.

In addition, we provide evaluation results for encoding scheme ranging from 2-1 in 3 data centers to 18-4 in 22 data centers to illustrate the flexibility of Giza as a front end.

}

For evaluation, we deploy Giza on top of the Microsoft Azure platform. The deployment reaches xxx data centers (xxx in North America, xxx in Europe and xxx in Asia). In each data center, we create a set of virtual machines as Giza nodes. We also create a number of Azure storage accounts. These storage accounts are local to individual DCs and allow access to Azure Blob storage and Azure Table storage in the DCs.

We form Giza storage account as a collection of the local Azure storage accounts. We also specify the erasure coding scheme for each account. For example, a North American only Giza storage account may apply $2+1$ erasure coding and consist of 4 data centers in the US, as xxx, xxx, xxx and xxx. A global Giza storage account may apply same coding and consist of 4 data centers across 3 continents, as xxx, xxx, xxx and xxx.

As described in section~\ref{sec:design}, all the Giza nodes are stateless. Upon receiving {\em get} or {\em put} requests, the Giza nodes execute the data and the metadata paths to read or write data objects.

\comment{

\subsection{Setup}
For our experiments, we deploy a single azure virtual machine (16 cores, 56 GB of RAM, 800 GB SSD, and gigabit ethernet) and set up a local storage account for each data center. For each storage account, we deploy a Blob storage and a Table storage and set the replication level for both to locally redundant. Each virtual machine runs a giza node and can serve requests from the users. Since we only tolerate a single data center failure, the number of giza nodes participating in the metadata phase is fixed at 3 for the classical paxos implementation and 4 for the fast paxos implementation. If the data path involves sending more than 3 or 4 data fragments, this set of Giza node is a subset of the total number of Giza nodes with the other Giza nodes only serving data path requests.The role of the Giza nodes in each data center and how many is determined by the clients per our use case.

To set up CockroachDB, we use the same azure virtual machine instances and run a single CockroachDB node. We followed the recommended production settings by the developers of CockroachDB when deploying these instances. For example, on the same virtual machine, we also run NTP to provide moderately accurate time to preserve data consistency. Other optimizations can be found on the CockroachDB website. We only benchmark CockraochDB against Giza in the 3 dc cluster scenario since we want the fault tolerance level to be the same for the comparisons.
Since variability in latency is a factor when benchmarking cloud storage, we run all our experiments at approximately the same time.
Since latency is an issue, we run all our experiments at around the same time.
We experimented with different erasure coding schemes
For all experiments, we deployed a single virtual machine (16 cores, 56 GB of RAM, 800 GB SSD, and gigabit ethernet) for each geographical region. We use the same virtual machines for setting up the Cassandra and CockroachDB clusters. The client issuing the requests runs on one of the virtual machine that is also part of the cluster. 
To set up Giza, we also had to deployed both a table service and a blob service provided by the cloud service platform. The granularity of replication for these services varies from provider to provider but we always choose the replication level to match that of the regional replication. This means that as long as there’s no dc outage, the data would not be lost. For each data center, we run a Giza node frontend with the virtual machine. The Giza node can service requests from a client running in the virtual machine. In addition, requests to its local table and blob storage from other Giza nodes also go through the Giza node frontend in the form of an RPC call. This is to avoid unecessary WAN round trips when dealing with complicated table and blob storage operations. 

How did we set up Cassandra?

This is how we set up Cassandra.

How did we set up CockroachDB?

We run a CockroachDB cluster spanning all the data centers included in the experiment where each node correspond to the virtual machine running on each geographical region. We use a single database of CockroachDB to emulate our Giza read and write path. The database contains multiple tables, a metadata table and multiple data tables corresponding to their respective region, and the tables differ in their replication level. The metadata table is replicated according to the fault tolerance level, i.e 2 if the the cluster can tolerate one data center outage [what about the location?]. The data table is replicated once only at the node of the respective region.
*machines*

We generate our workloads using the YCSB benchmark. Since our systems function as key value blob storage, we only require one field, the blob value, for each key. The probability that a given key will be accessed is given by the Zipfian distribution. We evaluated our systems with different blob value sizes. The smaller sizes include 64KB, 128KB, 256KB, and 512 KB while the bigger sizes include 1MB, 2MB, 4MB, and 8MB. We also evaluated them under different read/write ratios, including equal-shares and all writes.

\subsection{Metadata Path Optimization}
\sm {
  Hey Shuai, here I will provide the graph compaing the two metadata paths and the datapath to illustrate that this optimization makes the metadata path the non-dominant path in most cases. I will also have a table with the latency breakdown. The motivation for this is to show that such an optimization can allow users to tune the latency performance by changing the erasure coding scheme. This would not be possible it we use the two round trip. I will provide different latency measurements (95th, 75th, 50th percentile, as well as average with error bar so we can choose one later. This is true for all latency results I will provide)
}
We measure Giza's latency performance by running 1000 write and read operations with a single threaded Giza node.We vary the object size (1mb and 4mb) and compared different configurations. 
The 95th percentile latency result is shown in Figure 4. The four configurations we chose are 3 data centers in the US (2-1 erasure coding scheme), 3 data centers across the world, 7 data centers in the US (6-1 erasure coding scheme), and 7 data centers across the world. Both the configuration (i.e distance between data centers) and the size of the object impact latency peformance. Larger objects incur higher latency in both the vm to vm transfer and the local blob storage access. This can explain the similar put latency results between the 3 US data center configuration and the 7 US data center configuration. Even though in the 3 US data center configuration, the distance between the data centers are much shorter (Iowa, Illinois, and West Central), each data fragment is 2MB. We can see in Table 3 that this increases the latency performance for the data path. In the 7 US data center configuration, each data fragment is just 700 KB, which reduces latency even when the data centers are further away. 

}

\subsection{Giza Latency}

In our workloads, the predominant object size is 4MB. Hence, we focus on the {\em put} and {\em get} latency of 4MB objects in the evaluation. For every data point reported in this section, we repeat the corresponding Giza operation 1000 times and then calculate the latency at desired percentile. The global Giza storage account with 4 DCs is used for this evaluation.

\subsubsection{Optimizing Put Data Path}

add a bar plot comparing direct remote blob latency vs VM tunneling latency (use 50-percentile with error bars).

\subsubsection{Optimizing Put Metadata Path}

add a bar plot comparing Paxos vs Fast Paxos

\subsubsection{Giza Put Latency}

add a bar plot comparing baseline (one WAN RTT + storage), sequential data + Paxos, parallel data + Paxos, optimized

\subsubsection{Giza Read Latency}

add a bar plot comparing baseline (one WAN RTT + storage), sequential data + Paxos, optimized

\comment{

% \begin{figure}[!h]
% \centering
%   \subfloat[Giza Put 99th Percentile]{\includegraphics[width=0.5\textwidth]{images/write_latency}\label{fig:f1}}
%   \subfloat[Giza Get 99th Percentile]{\includegraphics[width=0.5\textwidth]{images/write_latency}\label{fig:f2}}
% \caption{Comparison of latency of the four configurations}
% \end{figure}
\subsection {Different Configurations}
\sm {
  In this section, I will provide a latency graph (put and get) of all the 4 different configurations. The x axis is the size of the objects and the y axis is the latency. This section is to illustrate the trade off between storage efficiency and read latency. 
}

}

\subsection{Footprint Impact}

Giza offers customers the flexibility to choose the set of data centers, as well as the erasure coding schemes. We evaluate 4 different Giza storage accounts, with the cross combination of $2+1$ vs $6+1$ erasure coding and all data centers in North America vs across 3 continents.

add a bar plot for put latency comparing the 4 configurations

add a bar plot for get latency comparing the 4 configurations

\subsection{Comparing Giza with CockroachDB}

To set up CockroachDB, we use the same azure virtual machine instances and run a single CockroachDB node. We followed the recommended production settings by the developers of CockroachDB when deploying these instances. For example, on the same virtual machine, we also run NTP to provide moderately accurate time to preserve data consistency. Other optimizations can be found on the CockroachDB website. We only benchmark CockraochDB against Giza in the 3 dc cluster scenario since we want the fault tolerance level to be the same for the comparisons.
Since variability in latency is a factor when benchmarking cloud storage, we run all our experiments at approximately the same time.
Since latency is an issue, we run all our experiments at around the same time.
We experimented with different erasure coding schemes
For all experiments, we deployed a single virtual machine (16 cores, 56 GB of RAM, 800 GB SSD, and gigabit ethernet) for each geographical region. We use the same virtual machines for setting up the Cassandra and CockroachDB clusters. The client issuing the requests runs on one of the virtual machine that is also part of the cluster. 
To set up Giza, we also had to deployed both a table service and a blob service provided by the cloud service platform. The granularity of replication for these services varies from provider to provider but we always choose the replication level to match that of the regional replication. This means that as long as there’s no dc outage, the data would not be lost. For each data center, we run a Giza node frontend with the virtual machine. The Giza node can service requests from a client running in the virtual machine. In addition, requests to its local table and blob storage from other Giza nodes also go through the Giza node frontend in the form of an RPC call. This is to avoid unecessary WAN round trips when dealing with complicated table and blob storage operations. 

\sm {
  In this section, I will have two graphs. One comparing Giza's full write path with CockroachDB's full replication. Another one is comparing the metadata path with cockroach db. This is to show, hopefully, that the fast paxos scheme is better. I will probably have 3 graphs each making request from one of the three datacenters. Due to multipaxos, there might be extra latency for cockroachdb's case.
}
We benchmark the performance of Giza with CockroachDB in two cases. In the first case, we use CockroachDB as a geo-replicating blah blah blah. Here is the result.
In the second case, we used cockroachdb's transaction to simulate what we are doing with Giza. Blah blah blah, here is the result.
64K $\sim$ 16MB

X-axis: Value size
Y-axis: 50\% Read latency

X-axis: Value size
Y-axis: 90\% Read latency

X-axis: Value size
Y-axis: 99\% Read latency

Same for write

[adding cpu results in a table]

\subsection{Large object}
256MB $\sim$ 1GB

X-axis: Value size
Y-axis: Average Read latency

X-axis: Value size
Y-axis: Average Write latency


\subsection{Contention}

Fixed object size
X-axis: zipf coefficient
Y-axis: 50\%, 90\%, 99\% Read/Write Latency


\subsection{Real workload}
Table.