\section{Replica Configuration and Failure Recovery}

This section discusses two practical aspects of a {\name} deployment, namely,
how to configure which set of data centers to use for storing coded data
fragments and replicating metadata, and how to handle failures.

\subsection{Data center configuration}

\name supports a large number of paying customers, each of which is associated
with his/her own storage account.  Each \name account is configured with a
collection of three or more DCs that replicate the metadata and/or store the
coded data framents.  The number of DCs is determined by the coding rate. For example, if a 
6+1 erasure code is to be used, there should be 7 DCs in the configuration.  
Furthermore, 3 among the 7 DCs are designated as metadata DCs that replicate 
metadata in addition to storing coded data.
The choice of DCs is determined by user preferences.  For
example, an OneDrive-like application may create a number of {\name} accounts,
each of which stores data for users in a geographical region. The application
could set up these accounts to choose a few DCs that are close to that
region for better user experience.

The mapping from each \name account to its set of DCs is stored in a
separate configuration service external to the \name system.  To service
Get/Put requests for an account, \name retrieves the account's DC configuration
from the external service and caches the information for future requests for a
threshold period of time.

{\bf Changing DC Configuration.} 
The set of DCs for an account may be changed, either due to changing user
preferences or recovering from a failed DC.  Each new DC configuration for an
account is associated with a monotonically increasing identifier, view-id.
Since view change happens rarely, we keep history of all views.  The view-id is
attached to each version of the object metadata. To change to a new DC
configuration for an account, {\name} goes into a grace period for the account.
During this grace period, we attempt to invalidate the cached DC configuration
information in all {\name} nodes and to replicate the metadata for all objects
of an account to the new DCs. In the grace period, it is possible that
different nodes hold different DC configurations for the same account. They may
read or write different set of datacenters. If a {\name} node fails with its
current view, it re-reads the most recent view and recursively retries. The
grace period ends once the cached information in all {\name} nodes are
invalidated and the metadata have been replicated to the new metadata DCs.

The migration of coded data fragments from the old configuration of DCs to the
new ones can happen outside of the grace period and in the background. Whenever 
an object's fragments have been moved or re-generated in the new DCs, the migration worker 
writes a new version of metadata for the object.
%similar to the failure recovery process, except that the migration process
%needs to update the view id in the metadata. In case of coding reconfiguration,
%i.e. the application changes the number of data fragments in encoding, the
%migration process also needs to compute all the parity fragments for the new
%view.


\subsection{Failure Handling}

Failures in {\name} are mainly in two catogeries: {\name} node failures and
datacenter failures. Since each \name node is stateless, 
we can simply launch new machines to replace any failed ones.
Note that the new node does not need to wait until
failed ones to be shutdown.  Both the new and old nodes could be
alive in the systems. This is okay because the {\name} protocols already
guarantee safety in case of contention, thus no particular actions are needed
to force the end of life cycle of node suspected of failure.

The table service and blob storage service are reliable in the face of machine
failures within a DC.  When an entire DC becomes unavailable, such as a
temporary network partition or an entire datacenter breakdown, {\name} can
still serve Get/Put requests, albeit at degraded performance. When handling a Put, 
\name takes two cross-DC roundtrips instead of one,
because there are not enough DCs to make up a FastPaxos quorum when 1 out of 3 DCs
replicating the metadata is down.  When handling a Get, \name must read the parity 
fragment as well as data fragments to re-construct the entire data object.
%for 
%datacenter left. It is worth noticing that the threshold could be different for
%metadata path and data path. For metadata, a majority of alive datacenters
%are required. For data path, assume it writes to $N$ datacenters in total with
%$m$ data fragments. To tolerate $F$ datacenter failures, it needs to write
%to at least $F+m$ datacenters, which implies that $F<N-m$ (To tolerate $F$
%failures, you need to have at least $F$ parities).

When a datacenter comes back online after crash, it needs to update the
metadata rows in its table service and data fragments in the blob storage.
A number of recovering clients are launched to scan through its objects.
For each object, it issues a normal read request except that it does not
pre-fetch the data. If the local version match the global version, nothing
needs to be done; if the local version is behind, the recovering process
then reads the latest data fragments, re-calcuates the fragment that belongs
to the local datacenter and writes it to the blob storage consistently with
the metadata.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
