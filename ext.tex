\section{Failure Recovery}

%There are two categories of failures: node failures and DC failures.
%The cloud table service and blob storage service that {\name} depends upon have already 
%been designed and tested to be resilient in the face of machine failures within a DC. 
%Those services have also been deployed and extensively tested. 
%Since each \name node is stateless, handling node failure is simple: we can
%simply launch new machines to replace any failed ones. In this section we focus
%on how \name handles DC failures.

\name needs to cope with transient or permanent data center failures. \textit{\textbf{Since \name treats an entire data center as a fault domain, failures within a data center (server failures, network failures, etc...) are resolved by individual cloud object store and table store within each data center. (5, 9)}}

{\bf Transient DC failure:}
We broadly categorize transient DC failure to include temporary outages of the blob
and table storage service in a DC. 
Transient DC failure may be caused by a temporary network partition or power failure.
By design, {\name} can still serve {\em get} and {\em put} requests, albeit at degraded performance. 
For example, when handling {\em put} requests,
\name may take more than one cross-DC round trip,
because some of the DCs replicating the metadata are unavailable,
resulting in fewer DCs than required for a fast path quorum.
% When handling a Get, \name must read the parity fragment as well as
% data fragments to re-construct the entire data object.
% JinL: Get also needs to make sure metadata is the correct version, ref to the last section. 

When a data center recovers from transient failures,
it needs to catch up and update the fragments in its blob storage
and the metadata rows in its table storage. 
The process follows the Paxos learning algorithm~\cite{lamport01paxos}.
For each object, Giza issues a read request of the metadata without fetching the fragments.
If the local version matches the committed version, nothing needs to be done;
if the local version is behind,
the recovering process reads the fragments of all missing versions,
reconstructs corresponding missing fragments and stores them in the blob storage,
as well as updates the metadata row in the table storage.

\comment{

\sm {
JinL: What about new objects created during the transient failure of DCs. I felt that there need to be log that tracks object creation, as those objects can not be discovered by just scanning the local metadata table. Scanning remote metadata table is another option (expensive though). 
}
}

{\bf Permanent DC Failure:}
Although extremely rare, a data center may fail catastrophically. The blob and
table storage service within the DC may also experience long-term outages. We
categorize these all as permanent DC failure. 

\name handles permanent DC failure by employing logical DC names in storage accounts. The mapping between a logical DC name to a physical DC location is stored in a separate service external to \name. Upon a permanent DC failure, the same logical DC name is re-mapped from the failed DC to a healthy replacement. Giza metadata records logical DC names and therefore remains unchanged after the re-mapping. This is similar to DNS, where same domain name can be re-mapped to a different physical IP address. This way of handling failure is also reported in Chubby~\cite{chubby:osdi06}.

Upon the permanent DC failure, \name launches recovery coordinators to reconstruct the missing fragments and re-insert the metadata rows in the replacement DC. The procedure is similar to how \name handles transient failures yet may last much longer. The reconstruction is paced and prioritized based on demand, with sufficient cross-DC network bandwidth in-place to ensure timely recovery.

%% \section{Replica Configuration Failure Recovery}

%% This section discusses two practical aspects of a {\name} deployment, namely,
%% how to configure which set of data centers to use for storing coded data
%% fragments and replicating metadata, and how to handle failures.

%% \subsection{Data center configuration}
%% \label{subsec:config}

%% \name supports a large number of paying customers, each of which is associated
%% with his/her own storage account.  Each \name account is configured with a
%% collection of three or more DCs that replicate the metadata and/or store the
%% coded data framents.  The customer specifies a desired fault tolerance goal, and an acceptable cost
%% of storing the data. The configuration determines the number of DCs that the customer's data will be stored, which in turn determines coding rate. For example, if the customer wishes to protect against
%% one data center failure and is willing to pay $17\%$ extra for the protection,
%% a 6+1 erasure code is to be used, and there should be 7 DCs in the configuration.  
%% Furthermore, 3 among the 7 DCs are designated as metadata DCs that replicate 
%% metadata in addition to storing coded data.
%% The choice of DCs is determined by user preferences. For
%% example, OneDrive could choose the DC where the user makes the most data access and 
%% other closeby DCs for better peformance.

%% The mapping from each \name account to its set of DCs is stored in a
%% separate configuration service external to the \name system.  To service
%% Get/Put requests for an account, \name retrieves the account's DC configuration
%% from the external service and caches the information for future requests for a
%% threshold period of time.

%% {\bf Changing DC Configuration.} 
%% The set of DCs for an account may be changed, either due to changing user
%% preferences or recovering from a failed DC.  Each new DC configuration for an
%% account is associated with a monotonically increasing identifier, view-id.
%% Since configuration changes infrequently, we keep the history of all DC configurations.  The view-id is
%% attached to each version of the object metadata. To change to a new DC
%% configuration for an account, \name enters a grace period during which the
%% cached DC configuration information in all {\name} nodes are invalidated.
%% During the grace period, it is possible that some nodes use the old DC configuration
%% while others use the new one. To ensure metadata consistency in this scenario,
%% whenever \name writes a new version of the metadata whose previous version has
%% the old DC configuration, its Paxos implementation obtains quorums in {\em
%% both} the old and new configuration.  Therefore, DCs in the old and new
%% configuration must concurrently agree on the same metadata version.

%% The migration of coded data fragments from the old configuration of DCs to the
%% new ones can happen outside of the grace period and in the background. Whenever
%% an object's fragments have been moved or re-generated in the new DCs, the
%% migration worker writes a new version of metadata for the object.
%% Before the completion of data migration, it is possible that a {\em Get} operation 
%% fails to obtain metadata or data in the new configuration.  In this situation, 
%% {\name} retries the operation under the previous configuration.

%% \subsection{Failure Handling}

%% There are two categories of failures: {\name} node failures and DC failures.
%% Since each \name node is stateless, handling node failure is simple: we can
%% simply launch new machines to replace any failed ones.  We focus on how \name
%% handles DC failures.

%% {\bf Transient DC failure.}
%% We categorize the transient DC failure to include temporary outages of the blob
%% and table storage service in a DC.  Giza copes with transient DC failure by
%% design.


%% The table service and blob storage service are reliable in the face of machine
%% failures within a DC.  When an entire DC becomes unavailable, such as a
%% temporary network partition or an entire datacenter breakdown, {\name} can
%% still serve Get/Put requests, albeit at degraded performance. When handling a Put, 
%% \name takes two cross-DC roundtrips instead of one,
%% because there are not enough DCs to make up a FastPaxos quorum when 1 out of 3 DCs
%% replicating the metadata is down.  When handling a Get, \name must read the parity 
%% fragment as well as data fragments to re-construct the entire data object.


%% When a datacenter comes back online after crash, it needs to update the
%% metadata rows in its table service and data fragments in the blob storage.
%% A number of recovering clients are launched to scan through its objects.
%% For each object, it issues a normal read request except that it does not
%% pre-fetch the data. If the local version match the global version, nothing
%% needs to be done; if the local version is behind, the recovering process
%% then reads the latest data fragments, re-calcuates the fragment that belongs
%% to the local datacenter and writes it to the blob storage consistently with
%% the metadata.


%% {\bf Permanent DC Failure}
%% Data center may fail catastrophically. The blob and table storage service
%% within the DC may also experience long-term outages. We categorize these all as
%% permanent DC failure.

%% \name handles permanent DC failure by triggering a DC configuration change 
%% which replaces the failed DC with a new healthy one. This process is identical 
%% to that required for the configuration change (Sec~\ref{subsec:config}) except
%% \name also need to re-generate lost coded fragments instead of simply moving
%% existing ones.  Furthermore, we prioritize work done to handle permanent DC
%% failure over that for ordinary DC configuration change.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
