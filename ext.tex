\section{Replica Configuration and Failure Recovery}

This section discusses two practical aspects of a {\name} deployment, namely,
how to configure which set of data centers to use for storing coded data
fragments and replicating metadata, and how to handle failures.

\subsection{Data center configuration}
\label{subsec:config}

\name supports a large number of paying customers, each of which is associated
with his/her own storage account.  Each \name account is configured with a
collection of three or more DCs that replicate the metadata and/or store the
coded data framents.  The number of DCs is determined by the coding rate. For example, if a 
6+1 erasure code is to be used, there should be 7 DCs in the configuration.  
Furthermore, 3 among the 7 DCs are designated as metadata DCs that replicate 
metadata in addition to storing coded data.
The choice of DCs is determined by user preferences.  For
example, OneDrive could choose the DC where the user makes the most data access and 
other closeby DCs for better peformance.

The mapping from each \name account to its set of DCs is stored in a
separate configuration service external to the \name system.  To service
Get/Put requests for an account, \name retrieves the account's DC configuration
from the external service and caches the information for future requests for a
threshold period of time.

{\bf Changing DC Configuration.} 
The set of DCs for an account may be changed, either due to changing user
preferences or recovering from a failed DC.  Each new DC configuration for an
account is associated with a monotonically increasing identifier, view-id.
Since configuration changes infrequently, we keep the history of all DC configurations.  The view-id is
attached to each version of the object metadata. To change to a new DC
configuration for an account, \name enters a grace period during which the
cached DC configuration information in all {\name} nodes are invalidated.
During the grace period, it is possible that some nodes use the old DC configuration
while others use the new one. To ensure metadata consistency in this scenario,
whenever \name writes a new version of the metadata whose previous version has
the old DC configuration, its Paxos implementation obtains quorums in {\em
both} the old and new configuration.  Therefore, DCs in the old and new
configuration cannot concurrently agree on the same metadata version.

The migration of coded data fragments from the old configuration of DCs to the
new ones can happen outside of the grace period and in the background. Whenever
an object's fragments have been moved or re-generated in the new DCs, the
migration worker writes a new version of metadata for the object.
Before the completion of data migration, it is possible that a {\em Get} operation 
fails to obtain metadata or data in the new configuration.  In this situation, 
{\name} retries the operation under the previous configuration.
%similar to the failure recovery process, except that the migration process
%needs to update the view id in the metadata. In case of coding reconfiguration,
%i.e. the application changes the number of data fragments in encoding, the
%migration process also needs to compute all the parity fragments for the new
%view.


\subsection{Failure Handling}

There are two categories of failures: {\name} node failures and DC failures.
Since each \name node is stateless, handing node failure is simple: we can
simply launch new machines to replace any failed ones.  We focus on how \name
handles DC failures.

{\bf Transient DC failure.}
We categorize the transient DC failure to include temporary outages of the blob
and table storage service in a DC.  Giza copes with transient DC failure by
design.


The table service and blob storage service are reliable in the face of machine
failures within a DC.  When an entire DC becomes unavailable, such as a
temporary network partition or an entire datacenter breakdown, {\name} can
still serve Get/Put requests, albeit at degraded performance. When handling a Put, 
\name takes two cross-DC roundtrips instead of one,
because there are not enough DCs to make up a FastPaxos quorum when 1 out of 3 DCs
replicating the metadata is down.  When handling a Get, \name must read the parity 
fragment as well as data fragments to re-construct the entire data object.
%for 
%datacenter left. It is worth noticing that the threshold could be different for
%metadata path and data path. For metadata, a majority of alive datacenters
%are required. For data path, assume it writes to $N$ datacenters in total with
%$m$ data fragments. To tolerate $F$ datacenter failures, it needs to write
%to at least $F+m$ datacenters, which implies that $F<N-m$ (To tolerate $F$
%failures, you need to have at least $F$ parities).

When a datacenter comes back online after crash, it needs to update the
metadata rows in its table service and data fragments in the blob storage.
A number of recovering clients are launched to scan through its objects.
For each object, it issues a normal read request except that it does not
pre-fetch the data. If the local version match the global version, nothing
needs to be done; if the local version is behind, the recovering process
then reads the latest data fragments, re-calcuates the fragment that belongs
to the local datacenter and writes it to the blob storage consistently with
the metadata.


\section{Coping with DC Failure}

\ch{The below is written before the latest section 4 update.}

Giza is designed to cope with data center failures,
which broadly include transient DC unavailability or permanent DC failure,
as well as temporary or long-term outages of the blob and table storage service within individual DCs.

\subsection{Transient DC Failure}

We categorize the transient DC failure to include temporary outages of the blob
and table storage service in a DC.  Giza copes with transient DC failure by
design. 

To tolerate single DC failure, each \name storage account is configured with
one additional DC.  For example, With the $2 + 1$ erasure coding scheme, the
corresponding storage account is configured with a set of 4 DCs.  On the data
path, each object is encoded into 3 fragments and any 3 out of the 4 DCs can be
chosen to store them.  On the metadata path, Paxos commits the metadata to any
3 out of the 4 DCs. Note that both the fast and the slow quorum have the same
size, that is equal to 3.

When single DC becomes unavailable, or simply slow, {\em put} operation
continues without much impact. It completes both the data and metadata paths by
storing coded fragments and metadata in the rest 3 DCs. {\em Get} operation
isn't affected much either. Given the hint from {\tt known committed versions},
to confirm the latest version, the {\em get} operation typically only reads the
metadata from one non-local DC, in addition to a local DC. Hence, the {\em get}
operation completes both the data and the metadata paths by reaching from 2
DCs.

Similarly, with the $6 + 1$ erasure coding scheme, a corresponding storage
account consists of 8 DCs. The data path is free to choose any 7 out of the 8
DCs. Metadata is limited to a subset of 4 DCs specified for the storage
account. Individual metadata paths are still free to choose any 3 out of the 4
DCs.

{\bf Permanent DC Failure}
Data center may fail catastrophically. The blob and table storage service
within the DC may also experience long-term outages. We categorize these all as
permanent DC failure.

\name handles permanent DC failure by triggering a DC configuration change 
which replaces the failed DC with a new healthy one. This process is identical 
to that required for the configuration change (Sec~\ref{subsec:config}) except
\name also need to re-generate lost coded fragments instead of simply moving
existing ones.  Furthermore, we prioritize work done to handle permanent DC
failure over that for ordinary DC configuration change.

%Permanent DC failure typically triggers a DC configuration change 
%to migrate data and metadata from the failed DC to a new healthy one.
%This, however, can be a subtle process.
%Each storage account is configured with a set of DCs, which need to be updated to replace the failed DC with the new one.
%For each data object, its missing fragment originally stored in the failed DC needs to be reconstructed and stored in the new DC.
%Moreover, its metadata needs to be updated,
%not only to reflect the new fragment location, but also the change of the set of DCs for the Paxos algorithm.
%The latter is referred to as Paxos view change.
%It is very tricky to correctly implement the Paxos view change mechanism, in light of continuous {\em get} and {\em put} operations, while at the same time dealing with the permanent DC failure.
%
%Fortunately, permanent DC failure is an extremely rare event
%which allows Giza to resort to a much simplified approach.
%Upon permanent DC failure, a new storage account is created to replace the old storage account affected by the failed DC.
%The new storage account copies the set of DCs and replaces the failed DC with the new one.
%Giza then enumerates the old storage account and copies every object to the new storage account.
%The data path of copying an object to the new storage account is optimized to leave existing coded fragments unchanged, while only reconstruct and store missing fragments in the new DC.
%In other words, Giza avoids unnecessary writes to the blob storage whenever possible.
%To store metadata, the new storage account creates a new set of metadata tables.
%The metadata path of copying the object therefore involves committing new metadata in the corresponding rows in the new metadata tables. The object is deleted from the old storage account after being successfully copied to the new storage account.
%

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
