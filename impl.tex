\section{Implementation}
\label{sec:impl}

Giza is implemented in C++ and uses Azure Blob and Table storage to store fragments and metadata.
The global footprint of Azure Storage allows for experimenting with a wide range of erasure coding parameters.

The \name design relies on atomic conditional write.
For Azure Table, we leverage its ETag mechanism. An unique ETag 
is generated by the table service for every write. To implement an atomic
conditional write, a \name node first reads the ETag of a table row.
It then performs the condition check and issues the
write request together with the ETag. Azure Table rejects the
write request if the ETag in the request does not match the one in the table,
which could only occur due to a concurrent write to the row.

To minimize latency, the \name node \emph{delegates} its conditional write requests
to remote \name nodes, which reside in the same DCs as the tables and act as proxies
in reading the ETag and writing the local table row.

\comment{
In our implementation, \name nodes in one datacenter also serve as proxies for
other nodes in other datacenters during blob storage accesses.  Proxying for
blob storage is due to our observation that the cross datacenter VM-to-VM
latency is much less variant over time than the cross-DC VM-to-storage latency.
We chose this proxy-style to better study and analyze the performance of \name.

\sm{
JinL: \em{Delegating conditional write request } conflicts with the statement below in Section 3. 
In \name, we use the cloud tables as the
acceptors and implement the acceptor's logic as {\em atomic conditional updates
} to the table. This requires some modification of the proposer logic, which is
implemented at each \name node.
If we do believe that we will to delegate conditional table update request, then \name node is the acceptor. The related
Section needs to be rewritten. 

}
}

%We made this implementation decision for two reasons. First, in ordere to implement the paxos logic using the underlying table interface, multiple requests might be necessary. This would incur unnecessary cross WAN round trips. In our case with Azure table, we implement the atomic updates to the paxos acceptors by using the provided Etags. When updating an acceptor entry, Giza first reads the entry and its associated etag. It then decides whether entry's value can be updated. If this is the case, Giza makes a replace entity request that only succeeds if the entry's etag value is still the same. During concurrent requests, this process can repeat multiple times. Second, we make the assumption that all network traffics are routed over the public internet. While this doesn't have to be the case since some cloud providers do offer dedicated private connection (e.g Azure Express Route), setting up private connections across wide area network may be too costly to be an option. As such, cross regional vm to storage latency performance can be significantly degraded during peak hours. We observed that latency can be reduced by first sending metadata and data to the vm of the targeted data center and then having the vm send data to its local blob and table storage. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
