\section{Implementation}
\label{sec:impl}

We implement Giza in C++ and use Microsoft Azure blob and table storage to store data fragments and metadata respsectively.
Azure currently offers the largest number of data centers across different geographical regions. This allows for experimenting 
with a wider range of coding schemes.
% Giza is designed to be invariant to the underlying cloud services and could also use Amazon's S3 and DynamoDB. 
%We implemented the Azure storage and table interface using the official Azure Storage Client Library for C++ (2.4.0). 
%We use the Jerasure and GF-Complete~\cite{Jerasure} as our erasure code libraries.
%However, Giza allows the flexibility for using different coding schemes as well.

%{\bf Implementing conditional write.}

The \name design depends on the conditional write feature provided by the underlying 
table service.
For Azure table, we leverage its ``ETag'' mechanism. An unique ETag 
is generated by the table service for every write.  To implement arbitary atomic
conditional write, a \name node first reads the corresponding table row to
obtain the ETag, performs the required condition check, and then issues the
write request together with this ETag.  Azure table service rejects the
write request if the current ETag of the row does not match the one in the
request, which happens if another \name node has concurrently written to the row.  

Performing conditional write using ETags 
incurs an extra cross-DC roundtrip to read the row from a remote table.
To optimize the latency, we allow a \name node to \emph{delegate} its 
conditional write request to a remote table through another node 
residing in the same DC as the table.

\comment{
In our implementation, \name nodes in one datacenter also serve as proxies for
other nodes in other datacenters during blob storage accesses.  Proxying for
blob storage is due to our observation that the cross datacenter VM-to-VM
latency is much less variant over time than the cross-DC VM-to-storage latency.
We chose this proxy-style to better study and analyze the performance of \name.
}

%We made this implementation decision for two reasons. First, in ordere to implement the paxos logic using the underlying table interface, multiple requests might be necessary. This would incur unnecessary cross WAN round trips. In our case with Azure table, we implement the atomic updates to the paxos acceptors by using the provided Etags. When updating an acceptor entry, Giza first reads the entry and its associated etag. It then decides whether entry's value can be updated. If this is the case, Giza makes a replace entity request that only succeeds if the entry's etag value is still the same. During concurrent requests, this process can repeat multiple times. Second, we make the assumption that all network traffics are routed over the public internet. While this doesn't have to be the case since some cloud providers do offer dedicated private connection (e.g Azure Express Route), setting up private connections across wide area network may be too costly to be an option. As such, cross regional vm to storage latency performance can be significantly degraded during peak hours. We observed that latency can be reduced by first sending metadata and data to the vm of the targeted data center and then having the vm send data to its local blob and table storage. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
